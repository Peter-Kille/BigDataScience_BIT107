{"title":"Threads, MPI and embarrassingly parallel","markdown":{"yaml":{"title-block-style":"default","title-block-banner":"darkred","title-block-banner-color":"white","title":"Threads, MPI and embarrassingly parallel","subtitle":"Making my job faster","author":"Prof. Peter Kille","date":"today","affiliation":"Cardiff University"},"headingText":"Threads, MPI and embarrassingly parallel","containsRefs":false,"markdown":"\n\n<body style=\"background-color:gray99;\">\n\n```{r include=FALSE}\nknitr::opts_chunk$set(eval = FALSE)\n```\n\n![](Images/logo.jpg){width=\"10%\"}\n\n\n## Parallel Jobs\n\nThere are a number of different scenarios in which you may want to parallelize your job:\n\n-   Embarrassingly parallel\n-   MPI - a multi-process application (possibly across multiple compute hosts)\n-   multi-threading - shared memory using OpenMP or perhaps pthreads.\n-   multiple instances of a single application.\n-   ...plus more scenarios but are probably out of scope of this tutorial.\n\n## Embarrassingly parallel\n\nMany processes in genomics do the same task on large arrays of data. One of the simplest way of speeding up the process is to split up the input files and perfrom the same task multiple times at the same time - this is called an \\_**'Embarrassingly parallel'** task.\n\nLets do this for the blast example that we started in the last task. We can investigate whether there are any further methods of improving performance, and attempt to find a improvemet in the initial blast job's solution to reduce the time taken. As it turns out, it is possible in this situation to split the input fasta file into a number of sections, and have an independent job acting on each of those sections. Each independent job could then be parallelized, say over 8 threads, and all jobs can run concurrently.\n\nWe create a job script (mammoth_8thread_split_1of4.sh) that will run blast command on the first file-section only.\n\nWe perform the following sequence of commands, first splitting the input fasta file into 4 parts, then creating the 4 independent job-scripts, and submit the jobs.\n\n```{bash}\n## enter a interactive Job\nsrun -c 4 --mem=8G -p defq --pty bash\ncd ~/scratch/blast_test/\n\n## a new folder\nmkdir split-files4\ncd split-files4/\ncp ~/TAIR10_pep_4000.fasta  .\n#\n## split the fasta file into 4 equal(ish) sections\ncurl -L https://raw.githubusercontent.com/gmarnellos/Trinotate_example_supplement/master/split_fasta.pl | perl /dev/stdin  TAIR10_pep_4000.fasta  TAIR10_pep.vol 1000\n```\n\ncreate script that will spawn blast_jobs (use nano or vi) - spawn_blastp.sh\n\n```{bash}\n!#/bin/bash\n\nfor i in {1..4};do\n\nexport i\n\nsbatch blastp.sh\n\ndone\n```\n\nmake script executable `chmod +x spawn_blastp.sh`\n\nEdit blast job so that each time it is called it uses a different section of the query file\n\n```{bash}\n#!/bin/bash\n#SBATCH --partition=defq       # the requested queue\n#SBATCH --nodes=1              # number of nodes to use\n#SBATCH --tasks-per-node=1     # \n#SBATCH --cpus-per-task=8      #   \n#SBATCH --mem-per-cpu=11500    # in megabytes, unless unit explicitly stated\n#SBATCH --error=%J.err         # redirect stderr to this file\n#SBATCH --output=%J.out        # redirect stdout to this file\n##SBATCH --mail-user=[insert email address]@Cardiff.ac.uk  # email address used for event notification\n##SBATCH --mail-type=start                                 # email on job start  \n##SBATCH --mail-type=end                                   # email on job end\n##SBATCH --mail-type=fail                                  # email on job failure\n\necho \"Usable Environment Variables:\"\necho \"=============================\"\necho \"hostname=$(hostname)\"\necho \\$SLURM_JOB_ID=${SLURM_JOB_ID} \necho \\$SLURM_NTASKS=${SLURM_NTASKS}\necho \\$SLURM_NTASKS_PER_NODE=${SLURM_NTASKS_PER_NODE}\necho \\$SLURM_CPUS_PER_TASK=${SLURM_CPUS_PER_TASK}\necho \\$SLURM_JOB_CPUS_PER_NODE=${SLURM_JOB_CPUS_PER_NODE}\necho \\$SLURM_MEM_PER_CPU=${SLURM_MEM_PER_CPU}\n\ncat $0\n\nmodule load blast/2.12.0\n\nindir=\"/mnt/scratch/${USER}/blast_test/split-files4\"\n\ndbdir=\"/mnt/scratch/${USER}/blast_test\"\n\noutdir=\"/mnt/scratch/${USER}/blast_test/split-files4\"\n\ntime blastp -num_threads ${SLURM_CPUS_PER_TASK} \\\n            -query \"${indir}/TAIR10_pep.vol.${i}.fasta\" \\\n            -task blastp \\\n            -num_descriptions 16 \\\n            -num_alignments 1 \\\n            -db ${dbdir}/TAIR10_pep \\\n            -out \"${outdir}/blastp_vol${i}_cpu${SLURM_CPUS_PER_TASK}_job${SLURM_JOBID}.txt\"\n```\n\nPerform file-splitting procedure for both a 2-split and a 4-split of the original fasta file. The 'time-to-solution' results are added to the original benchmark chart. We assume that all jobs run concurrently, and we take the wall-time for the longest job\n\n::: {.callout-note collapse=\"true\"}\n## MPI Jobs\n\nOur example MPI job is based on a quantum espresso calculation. This script utilises the srun command, which is part of the slurm family of tools to run a parallel job on a cluster\n\n```{bash}\n#!/bin/bash\n#SBATCH --partition=mammoth    # the requested queue\n#SBATCH --job-name=qe_mpi      # name the job           \n#SBATCH --nodes=1              # number of nodes to use\n#SBATCH --ntasks=32            # total number of tasks (processes)\n#SBATCH --mem-per-cpu=100      # in megabytes, unless unit explicitly stated\n#SBATCH --error=%J.err         # redirect stderr to this file\n#SBATCH --output=%J.out        # redirect stdout to this file\n##SBATCH --mail-user=[insert email address]@Cardiff.ac.uk  # email address used for event notification\n##SBATCH --mail-type=end                                   # email on job end\n##SBATCH --mail-type=fail                                  # email on job failure\n\nmodule load  qe/6.0\n\necho \"Usable Environment Variables:\"\necho \"=============================\"\necho \"hostname=$(hostname)\"\necho \\$SLURM_JOB_ID=${SLURM_JOB_ID}\necho \\$SLURM_NTASKS=${SLURM_NTASKS}\necho \\$SLURM_NTASKS_PER_NODE=${SLURM_NTASKS_PER_NODE}\necho \\$SLURM_CPUS_PER_TASK=${SLURM_CPUS_PER_TASK}\necho \\$SLURM_JOB_CPUS_PER_NODE=${SLURM_JOB_CPUS_PER_NODE}\necho \\$SLURM_MEM_PER_CPU=${SLURM_MEM_PER_CPU}\necho \"module list:\"\nmodule list 2>&1\n\n# Some of these environment variables are utilised by the qe executable itself\nexport ESPRESSO_DATAPATH=~/classdata/REFS/slurm/slurm_examples/example2_mpi/\nexport ESPRESSO_PSEUDO=${ESPRESSO_DATAPATH}\nexport ESPRESSO_TMPDIR=${ESPRESSO_DATAPATH}/${SLURM_JOB_ID}\n\n# handy to place this in job output for future reference...\ncat ${ESPRESSO_DATAPATH}/atom.in\n\n# execute the parallel job (we also time it)\ntime srun -n ${SLURM_NTASKS} pw.x < ${ESPRESSO_DATAPATH}/atom.in > atom.job${SLURM_JOB_ID}.out\n```\n\nThe job requests 32 cores to be allocated, and runs the srun command with the argument -n \\${SLURM_NTASKS} which tells srun to spawn the mpi-job with the total number of processes requested. Quantum Espresso utilises the environment variable ESPRESSO_TMPDIR which points to a temporary folder. We design this in our slurm script to point to a subfolder.\n\nAn alternative storage location is the compute node's local storage. This can improve runtime I/O performance. However, local storage on the compute nodes is limited (Gigabytes not Terabytes), and it's availability is a little hidden from the user, so take care not to fill up the disk(!) and remove all files from the compute node's local storage within the job script (your only access to the compute node's /local folder is via the slurm script). An alternative job script which utilises a compute node's /local storage is provided on the gomphus server /mnt/clusters/sponsa/data/classdata/Bioinformatics/REFS/slurm/slurm_examples/example2_mpi/example2_mpi_localstorage.sh.\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Threaded Jobs\n\nA number of popular bioinformatics software are capable of parallelising execution using threads (usually OpenMP or pthreads). This parallelisation method does not normally use distributed memory, so the application will need to be run on a single node. Our threaded example slurm-script is based on BLAST+. The job script is listed:\n\n```{bash}\n#!/bin/bash\n#SBATCH --partition=defq\n#SBATCH --nodes=1\n#SBATCH --tasks-per-node=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem-per-cpu=2000\n#SBATCH --error=%J.err\n#SBATCH --output=%J.out\n##SBATCH --mail-type=end\n##SBATCH --mail-user=[your.email@address]\n\n\n# Example slurm script\n#  This script is a little wasteful of resources,\n#  but demonstrates a simple pipeline.\n#  \n#  For a more efficient use of resources, please consider\n#  running the pipeline as a series of jobs (chain-jobs).\n\n\necho \"Usable Environment Variables:\"\necho \"=============================\"\necho \"hostname=$(hostname)\"\necho \\$SLURM_JOB_ID=${SLURM_JOB_ID}\necho \\$SLURM_NTASKS=${SLURM_NTASKS}\necho \\$SLURM_NTASKS_PER_NODE=${SLURM_NTASKS_PER_NODE}\necho \\$SLURM_CPUS_PER_TASK=${SLURM_CPUS_PER_TASK}\necho \\$SLURM_JOB_CPUS_PER_NODE=${SLURM_JOB_CPUS_PER_NODE}\necho \\$SLURM_MEM_PER_CPU=${SLURM_MEM_PER_CPU}\necho \"module list:\"\nmodule list 2>&1\n\nDATAFOLDER=~/classdata/REFS/slurm/slurm_examples/example3_pipeline\n\n\n### The data used in this pipeline has already been downloaded and stored in $DATAFOLDER.\n### Here are the commands used to download the data...\n# cd $DATAFOLDER\n# curl -LO https://data.broadinstitute.org/Trinity/Trinotate_v2.0_RESOURCES/uniprot_sprot.trinotate_v2.0.pep.gz\n# gunzip uniprot_sprot.trinotate_v2.0.pep.gz\n# curl -LO https://data.broadinstitute.org/Trinity/Trinotate_v2.0_RESOURCES/Pfam-A.hmm.gz\n# gunzip Pfam-A.hmm.gz\n# curl -L -o Trinotate.sqlite.gz https://data.broadinstitute.org/Trinity/Trinotate_v2.0_RESOURCES/Trinotate.sprot_uniref90.20150131.boilerplate.sqlite.gz\n# gunzip Trinotate.sqlite.gz\n# curl -LO ftp://ftp.ensembl.org/pub/release-82/fasta/mus_musculus/cdna/Mus_musculus.GRCm38.cdna.all.fa.gz\n# gunzip Mus_musculus.GRCm38.cdna.all.fa.gz\n# curl -LOgz https://github.com/gmarnellos/Trinotate_example_supplement/raw/master/mouse38_cdna.fa.gz\n# gunzip mouse38_cdna.fa.gz\n\n# Now we get on to the pipeline\n\n# make a link to all datafiles\nfor f in ${DATAFOLDER}/* ; do ln -s $f ; done\n\n#Index the SwissProt database for use with blast\n\nmodule load blast\nmakeblastdb -version\nmakeblastdb -in uniprot_sprot.trinotate_v2.0.pep -dbtype prot\nmodule unload blast\n\n# Prepare the Pfam database for use with hmmscan\nmodule load hmmer\nhmmpress -h\nhmmpress Pfam-A.hmm\nmodule load hmmer\n\n# Use Transdecoder to produce the most likely longest-ORF peptide candidates\nmodule load TransDecoder/v3.0.1\nTransDecoder.LongOrfs -t mouse38_cdna.fa\nTransDecoder.Predict -t mouse38_cdna.fa\nmodule unload TransDecoder/v3.0.1\n\nmodule load blast\nblastx -query mouse38_cdna.fa -db uniprot_sprot.trinotate.pep -num_threads ${SLURM_CPUS_PER_TASK} -max_target_seqs 1 -outfmt 6 > blastx.vol.outfmt6\nblastp -query mouse38_cdna.fa.transdecoder.pep -db uniprot_sprot.trinotate_v2.0.pep -num_threads ${SLURM_CPUS_PER_TASK} -max_target_seqs 1 -outfmt 6 > blastp.vol.outfmt6\nmodule unload blast\n\n# Identify protein domains\nmodule load hmmer/3.1b2\nhmmscan --cpu ${SLURM_CPUS_PER_TASK} --domtblout TrinotatePFAM.out Pfam-A.hmm mouse38_cdna.fa.transdecoder.pep > pfam.log\nmodule unload hmmer/3.1b2\n\n# Produce the Gene/Transcript relationship\ngrep \"^>\" Mus_musculus.GRCm38.cdna.all.fa   | perl -p -e 's/^>(\\S+).*\\s+gene:(ENSMUSG\\d+).*$/$2\\t$1/' > gene_transcript_map.txt\n\n# Now populate the sqlite database\nmodule load Trinotate/v3.0.1\nTrinotate Trinotate.sqlite init --gene_trans_map gene_transcript_map.txt --transcript_fasta mouse38_cdna.fa --transdecoder_pep mouse38_cdna.fa.transdecoder.pep\nTrinotate Trinotate.sqlite LOAD_swissprot_blastp blastp.vol.outfmt6\nTrinotate Trinotate.sqlite LOAD_swissprot_blastx blastx.vol.outfmt6\nTrinotate Trinotate.sqlite LOAD_pfam TrinotatePFAM.out\n# Create the annotation report\nTrinotate Trinotate.sqlite report -E 0.00001 > trinotate_annotation_report.xls\nmodule unload Trinotate/v3.0.1\n```\n\nThis is quite a busy job-script (and also inefficient on resources!). It runs through a number of steps, but some of those steps will utilise parallelisation via threading, and use the slurm environment variable SLURM_CPUS_PER_TASK to inform the application(s) of the correct number of threads.\n\nBut why is this job inefficient on resources? This particular job involves a number of steps: some utilising parallelisation, and some not; some memory-hungry, others not. The problem with this is that the job has allocated to it a set amount of resources (compute and memory), which is allocated to it for the lifetime of the job. But only at certain times in this job are the resources requested fully utilised. At all other times this job is running, the resources are allocated, but not used, and therefore making those resources unavailable to other jobs. This has a knock-on effect of increasing queue-times, and leaves expensive resources idle.\n\nA much more efficient way of running the same pipeline is to chain the job - split the pipeline into component parts and submit separate jobs for each of those parts. Each section of the pipeline (having its own job-script) is then free to allocate resources specific to that section of the pipeline. In the slurm world this is called job chaining, and has been exemplified in the next section using the same pipeline.\n\n## Job Chains and Job Dependency\n\nChaining jobs is a method of sequentially running dependent jobs. Our chain-job example is a pipeline of 6 separate job scripts, based on the blast+ pipeline of the previous section. We do not show the full six job-scripts here for brevity, but are available on the gomphus cluster under /mnt/clusters/sponsa/data/classdata/Bioinformatics/REFS/slurm/slurm_examples/example4_chain.\n\nSlurm has an option -d or --dependency that allows to specify that a job is only permitted to start if another job finished successfully.\n\nIn the folder (gomphus cluster) \\~/classdata/Bioinformatics/REFS/slurm/slurm_examples/example4_chain there are 6 separate job-scripts that need to be executed in a certain order. They are numbered in the correct pipeline order:\n\n```{bash}\n[user@gomphus ~]$ tree  ~/classdata/Bioinformatics/REFS/slurm/slurm_examples/example4_chain\n~/classdata/Bioinformatics/REFS/slurm/slurm_examples/example4_chain\n├── example4_chain-step1.sh\n├── example4_chain-step2.sh\n├── example4_chain-step3.sh\n├── example4_chain-step4.sh\n├── example4_chain-step5.sh\n├── example4_chain-step6.sh\n├── example4_submit_all.sh\n├── mouse38_cdna.fa\n├── Mus_musculus.GRCm38.cdna.all.fa\n├── Pfam-A.hmm\n├── pipeline1.sh\n├── Trinotate.sqlite\n└── uniprot_sprot.trinotate_v2.0.pep\n\n0 directories, 13 files\n```\n\nEach job is (importantly) commonly named using #SBATCH --job-name within each job-script. Also within this folder is a simple script (example4_submit_all.sh) that will execute the sbatch command on each of the job-scripts in the correct order:\n\n```{bash}\n#!/bin/bash:\n\nfor c in ~/classdata/REFS/slurm/slurm_examples/example4_chain/example4_chain-step?.sh ;\ndo\n sbatch -d singleton $c\ndone\n```\n\nThis sbatch command uses the -d singleton flag to notify slurm of the job-dependencies (all jobs must have the name job name defined by `#SBATCH --job-name [some constant name]`. At which point each submitted job will be forced to depend on successful completion of any previous job submitted by the same user, and with the same job-name. The full pipeline of 6 jobs will now run to completion, with no further user-intervention, making efficient use of the available resources.\n:::\n","srcMarkdownNoYaml":"\n\n<body style=\"background-color:gray99;\">\n\n```{r include=FALSE}\nknitr::opts_chunk$set(eval = FALSE)\n```\n\n![](Images/logo.jpg){width=\"10%\"}\n\n# Threads, MPI and embarrassingly parallel\n\n## Parallel Jobs\n\nThere are a number of different scenarios in which you may want to parallelize your job:\n\n-   Embarrassingly parallel\n-   MPI - a multi-process application (possibly across multiple compute hosts)\n-   multi-threading - shared memory using OpenMP or perhaps pthreads.\n-   multiple instances of a single application.\n-   ...plus more scenarios but are probably out of scope of this tutorial.\n\n## Embarrassingly parallel\n\nMany processes in genomics do the same task on large arrays of data. One of the simplest way of speeding up the process is to split up the input files and perfrom the same task multiple times at the same time - this is called an \\_**'Embarrassingly parallel'** task.\n\nLets do this for the blast example that we started in the last task. We can investigate whether there are any further methods of improving performance, and attempt to find a improvemet in the initial blast job's solution to reduce the time taken. As it turns out, it is possible in this situation to split the input fasta file into a number of sections, and have an independent job acting on each of those sections. Each independent job could then be parallelized, say over 8 threads, and all jobs can run concurrently.\n\nWe create a job script (mammoth_8thread_split_1of4.sh) that will run blast command on the first file-section only.\n\nWe perform the following sequence of commands, first splitting the input fasta file into 4 parts, then creating the 4 independent job-scripts, and submit the jobs.\n\n```{bash}\n## enter a interactive Job\nsrun -c 4 --mem=8G -p defq --pty bash\ncd ~/scratch/blast_test/\n\n## a new folder\nmkdir split-files4\ncd split-files4/\ncp ~/TAIR10_pep_4000.fasta  .\n#\n## split the fasta file into 4 equal(ish) sections\ncurl -L https://raw.githubusercontent.com/gmarnellos/Trinotate_example_supplement/master/split_fasta.pl | perl /dev/stdin  TAIR10_pep_4000.fasta  TAIR10_pep.vol 1000\n```\n\ncreate script that will spawn blast_jobs (use nano or vi) - spawn_blastp.sh\n\n```{bash}\n!#/bin/bash\n\nfor i in {1..4};do\n\nexport i\n\nsbatch blastp.sh\n\ndone\n```\n\nmake script executable `chmod +x spawn_blastp.sh`\n\nEdit blast job so that each time it is called it uses a different section of the query file\n\n```{bash}\n#!/bin/bash\n#SBATCH --partition=defq       # the requested queue\n#SBATCH --nodes=1              # number of nodes to use\n#SBATCH --tasks-per-node=1     # \n#SBATCH --cpus-per-task=8      #   \n#SBATCH --mem-per-cpu=11500    # in megabytes, unless unit explicitly stated\n#SBATCH --error=%J.err         # redirect stderr to this file\n#SBATCH --output=%J.out        # redirect stdout to this file\n##SBATCH --mail-user=[insert email address]@Cardiff.ac.uk  # email address used for event notification\n##SBATCH --mail-type=start                                 # email on job start  \n##SBATCH --mail-type=end                                   # email on job end\n##SBATCH --mail-type=fail                                  # email on job failure\n\necho \"Usable Environment Variables:\"\necho \"=============================\"\necho \"hostname=$(hostname)\"\necho \\$SLURM_JOB_ID=${SLURM_JOB_ID} \necho \\$SLURM_NTASKS=${SLURM_NTASKS}\necho \\$SLURM_NTASKS_PER_NODE=${SLURM_NTASKS_PER_NODE}\necho \\$SLURM_CPUS_PER_TASK=${SLURM_CPUS_PER_TASK}\necho \\$SLURM_JOB_CPUS_PER_NODE=${SLURM_JOB_CPUS_PER_NODE}\necho \\$SLURM_MEM_PER_CPU=${SLURM_MEM_PER_CPU}\n\ncat $0\n\nmodule load blast/2.12.0\n\nindir=\"/mnt/scratch/${USER}/blast_test/split-files4\"\n\ndbdir=\"/mnt/scratch/${USER}/blast_test\"\n\noutdir=\"/mnt/scratch/${USER}/blast_test/split-files4\"\n\ntime blastp -num_threads ${SLURM_CPUS_PER_TASK} \\\n            -query \"${indir}/TAIR10_pep.vol.${i}.fasta\" \\\n            -task blastp \\\n            -num_descriptions 16 \\\n            -num_alignments 1 \\\n            -db ${dbdir}/TAIR10_pep \\\n            -out \"${outdir}/blastp_vol${i}_cpu${SLURM_CPUS_PER_TASK}_job${SLURM_JOBID}.txt\"\n```\n\nPerform file-splitting procedure for both a 2-split and a 4-split of the original fasta file. The 'time-to-solution' results are added to the original benchmark chart. We assume that all jobs run concurrently, and we take the wall-time for the longest job\n\n::: {.callout-note collapse=\"true\"}\n## MPI Jobs\n\nOur example MPI job is based on a quantum espresso calculation. This script utilises the srun command, which is part of the slurm family of tools to run a parallel job on a cluster\n\n```{bash}\n#!/bin/bash\n#SBATCH --partition=mammoth    # the requested queue\n#SBATCH --job-name=qe_mpi      # name the job           \n#SBATCH --nodes=1              # number of nodes to use\n#SBATCH --ntasks=32            # total number of tasks (processes)\n#SBATCH --mem-per-cpu=100      # in megabytes, unless unit explicitly stated\n#SBATCH --error=%J.err         # redirect stderr to this file\n#SBATCH --output=%J.out        # redirect stdout to this file\n##SBATCH --mail-user=[insert email address]@Cardiff.ac.uk  # email address used for event notification\n##SBATCH --mail-type=end                                   # email on job end\n##SBATCH --mail-type=fail                                  # email on job failure\n\nmodule load  qe/6.0\n\necho \"Usable Environment Variables:\"\necho \"=============================\"\necho \"hostname=$(hostname)\"\necho \\$SLURM_JOB_ID=${SLURM_JOB_ID}\necho \\$SLURM_NTASKS=${SLURM_NTASKS}\necho \\$SLURM_NTASKS_PER_NODE=${SLURM_NTASKS_PER_NODE}\necho \\$SLURM_CPUS_PER_TASK=${SLURM_CPUS_PER_TASK}\necho \\$SLURM_JOB_CPUS_PER_NODE=${SLURM_JOB_CPUS_PER_NODE}\necho \\$SLURM_MEM_PER_CPU=${SLURM_MEM_PER_CPU}\necho \"module list:\"\nmodule list 2>&1\n\n# Some of these environment variables are utilised by the qe executable itself\nexport ESPRESSO_DATAPATH=~/classdata/REFS/slurm/slurm_examples/example2_mpi/\nexport ESPRESSO_PSEUDO=${ESPRESSO_DATAPATH}\nexport ESPRESSO_TMPDIR=${ESPRESSO_DATAPATH}/${SLURM_JOB_ID}\n\n# handy to place this in job output for future reference...\ncat ${ESPRESSO_DATAPATH}/atom.in\n\n# execute the parallel job (we also time it)\ntime srun -n ${SLURM_NTASKS} pw.x < ${ESPRESSO_DATAPATH}/atom.in > atom.job${SLURM_JOB_ID}.out\n```\n\nThe job requests 32 cores to be allocated, and runs the srun command with the argument -n \\${SLURM_NTASKS} which tells srun to spawn the mpi-job with the total number of processes requested. Quantum Espresso utilises the environment variable ESPRESSO_TMPDIR which points to a temporary folder. We design this in our slurm script to point to a subfolder.\n\nAn alternative storage location is the compute node's local storage. This can improve runtime I/O performance. However, local storage on the compute nodes is limited (Gigabytes not Terabytes), and it's availability is a little hidden from the user, so take care not to fill up the disk(!) and remove all files from the compute node's local storage within the job script (your only access to the compute node's /local folder is via the slurm script). An alternative job script which utilises a compute node's /local storage is provided on the gomphus server /mnt/clusters/sponsa/data/classdata/Bioinformatics/REFS/slurm/slurm_examples/example2_mpi/example2_mpi_localstorage.sh.\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Threaded Jobs\n\nA number of popular bioinformatics software are capable of parallelising execution using threads (usually OpenMP or pthreads). This parallelisation method does not normally use distributed memory, so the application will need to be run on a single node. Our threaded example slurm-script is based on BLAST+. The job script is listed:\n\n```{bash}\n#!/bin/bash\n#SBATCH --partition=defq\n#SBATCH --nodes=1\n#SBATCH --tasks-per-node=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem-per-cpu=2000\n#SBATCH --error=%J.err\n#SBATCH --output=%J.out\n##SBATCH --mail-type=end\n##SBATCH --mail-user=[your.email@address]\n\n\n# Example slurm script\n#  This script is a little wasteful of resources,\n#  but demonstrates a simple pipeline.\n#  \n#  For a more efficient use of resources, please consider\n#  running the pipeline as a series of jobs (chain-jobs).\n\n\necho \"Usable Environment Variables:\"\necho \"=============================\"\necho \"hostname=$(hostname)\"\necho \\$SLURM_JOB_ID=${SLURM_JOB_ID}\necho \\$SLURM_NTASKS=${SLURM_NTASKS}\necho \\$SLURM_NTASKS_PER_NODE=${SLURM_NTASKS_PER_NODE}\necho \\$SLURM_CPUS_PER_TASK=${SLURM_CPUS_PER_TASK}\necho \\$SLURM_JOB_CPUS_PER_NODE=${SLURM_JOB_CPUS_PER_NODE}\necho \\$SLURM_MEM_PER_CPU=${SLURM_MEM_PER_CPU}\necho \"module list:\"\nmodule list 2>&1\n\nDATAFOLDER=~/classdata/REFS/slurm/slurm_examples/example3_pipeline\n\n\n### The data used in this pipeline has already been downloaded and stored in $DATAFOLDER.\n### Here are the commands used to download the data...\n# cd $DATAFOLDER\n# curl -LO https://data.broadinstitute.org/Trinity/Trinotate_v2.0_RESOURCES/uniprot_sprot.trinotate_v2.0.pep.gz\n# gunzip uniprot_sprot.trinotate_v2.0.pep.gz\n# curl -LO https://data.broadinstitute.org/Trinity/Trinotate_v2.0_RESOURCES/Pfam-A.hmm.gz\n# gunzip Pfam-A.hmm.gz\n# curl -L -o Trinotate.sqlite.gz https://data.broadinstitute.org/Trinity/Trinotate_v2.0_RESOURCES/Trinotate.sprot_uniref90.20150131.boilerplate.sqlite.gz\n# gunzip Trinotate.sqlite.gz\n# curl -LO ftp://ftp.ensembl.org/pub/release-82/fasta/mus_musculus/cdna/Mus_musculus.GRCm38.cdna.all.fa.gz\n# gunzip Mus_musculus.GRCm38.cdna.all.fa.gz\n# curl -LOgz https://github.com/gmarnellos/Trinotate_example_supplement/raw/master/mouse38_cdna.fa.gz\n# gunzip mouse38_cdna.fa.gz\n\n# Now we get on to the pipeline\n\n# make a link to all datafiles\nfor f in ${DATAFOLDER}/* ; do ln -s $f ; done\n\n#Index the SwissProt database for use with blast\n\nmodule load blast\nmakeblastdb -version\nmakeblastdb -in uniprot_sprot.trinotate_v2.0.pep -dbtype prot\nmodule unload blast\n\n# Prepare the Pfam database for use with hmmscan\nmodule load hmmer\nhmmpress -h\nhmmpress Pfam-A.hmm\nmodule load hmmer\n\n# Use Transdecoder to produce the most likely longest-ORF peptide candidates\nmodule load TransDecoder/v3.0.1\nTransDecoder.LongOrfs -t mouse38_cdna.fa\nTransDecoder.Predict -t mouse38_cdna.fa\nmodule unload TransDecoder/v3.0.1\n\nmodule load blast\nblastx -query mouse38_cdna.fa -db uniprot_sprot.trinotate.pep -num_threads ${SLURM_CPUS_PER_TASK} -max_target_seqs 1 -outfmt 6 > blastx.vol.outfmt6\nblastp -query mouse38_cdna.fa.transdecoder.pep -db uniprot_sprot.trinotate_v2.0.pep -num_threads ${SLURM_CPUS_PER_TASK} -max_target_seqs 1 -outfmt 6 > blastp.vol.outfmt6\nmodule unload blast\n\n# Identify protein domains\nmodule load hmmer/3.1b2\nhmmscan --cpu ${SLURM_CPUS_PER_TASK} --domtblout TrinotatePFAM.out Pfam-A.hmm mouse38_cdna.fa.transdecoder.pep > pfam.log\nmodule unload hmmer/3.1b2\n\n# Produce the Gene/Transcript relationship\ngrep \"^>\" Mus_musculus.GRCm38.cdna.all.fa   | perl -p -e 's/^>(\\S+).*\\s+gene:(ENSMUSG\\d+).*$/$2\\t$1/' > gene_transcript_map.txt\n\n# Now populate the sqlite database\nmodule load Trinotate/v3.0.1\nTrinotate Trinotate.sqlite init --gene_trans_map gene_transcript_map.txt --transcript_fasta mouse38_cdna.fa --transdecoder_pep mouse38_cdna.fa.transdecoder.pep\nTrinotate Trinotate.sqlite LOAD_swissprot_blastp blastp.vol.outfmt6\nTrinotate Trinotate.sqlite LOAD_swissprot_blastx blastx.vol.outfmt6\nTrinotate Trinotate.sqlite LOAD_pfam TrinotatePFAM.out\n# Create the annotation report\nTrinotate Trinotate.sqlite report -E 0.00001 > trinotate_annotation_report.xls\nmodule unload Trinotate/v3.0.1\n```\n\nThis is quite a busy job-script (and also inefficient on resources!). It runs through a number of steps, but some of those steps will utilise parallelisation via threading, and use the slurm environment variable SLURM_CPUS_PER_TASK to inform the application(s) of the correct number of threads.\n\nBut why is this job inefficient on resources? This particular job involves a number of steps: some utilising parallelisation, and some not; some memory-hungry, others not. The problem with this is that the job has allocated to it a set amount of resources (compute and memory), which is allocated to it for the lifetime of the job. But only at certain times in this job are the resources requested fully utilised. At all other times this job is running, the resources are allocated, but not used, and therefore making those resources unavailable to other jobs. This has a knock-on effect of increasing queue-times, and leaves expensive resources idle.\n\nA much more efficient way of running the same pipeline is to chain the job - split the pipeline into component parts and submit separate jobs for each of those parts. Each section of the pipeline (having its own job-script) is then free to allocate resources specific to that section of the pipeline. In the slurm world this is called job chaining, and has been exemplified in the next section using the same pipeline.\n\n## Job Chains and Job Dependency\n\nChaining jobs is a method of sequentially running dependent jobs. Our chain-job example is a pipeline of 6 separate job scripts, based on the blast+ pipeline of the previous section. We do not show the full six job-scripts here for brevity, but are available on the gomphus cluster under /mnt/clusters/sponsa/data/classdata/Bioinformatics/REFS/slurm/slurm_examples/example4_chain.\n\nSlurm has an option -d or --dependency that allows to specify that a job is only permitted to start if another job finished successfully.\n\nIn the folder (gomphus cluster) \\~/classdata/Bioinformatics/REFS/slurm/slurm_examples/example4_chain there are 6 separate job-scripts that need to be executed in a certain order. They are numbered in the correct pipeline order:\n\n```{bash}\n[user@gomphus ~]$ tree  ~/classdata/Bioinformatics/REFS/slurm/slurm_examples/example4_chain\n~/classdata/Bioinformatics/REFS/slurm/slurm_examples/example4_chain\n├── example4_chain-step1.sh\n├── example4_chain-step2.sh\n├── example4_chain-step3.sh\n├── example4_chain-step4.sh\n├── example4_chain-step5.sh\n├── example4_chain-step6.sh\n├── example4_submit_all.sh\n├── mouse38_cdna.fa\n├── Mus_musculus.GRCm38.cdna.all.fa\n├── Pfam-A.hmm\n├── pipeline1.sh\n├── Trinotate.sqlite\n└── uniprot_sprot.trinotate_v2.0.pep\n\n0 directories, 13 files\n```\n\nEach job is (importantly) commonly named using #SBATCH --job-name within each job-script. Also within this folder is a simple script (example4_submit_all.sh) that will execute the sbatch command on each of the job-scripts in the correct order:\n\n```{bash}\n#!/bin/bash:\n\nfor c in ~/classdata/REFS/slurm/slurm_examples/example4_chain/example4_chain-step?.sh ;\ndo\n sbatch -d singleton $c\ndone\n```\n\nThis sbatch command uses the -d singleton flag to notify slurm of the job-dependencies (all jobs must have the name job name defined by `#SBATCH --job-name [some constant name]`. At which point each submitted job will be forced to depend on successful completion of any previous job submitted by the same user, and with the same job-name. The full pipeline of 6 jobs will now run to completion, with no further user-intervention, making efficient use of the available resources.\n:::\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"2.3_MPI_stupidly_parrellel.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.39","editor":"visual","theme":"cosmo","title-block-style":"default","title-block-banner":"darkred","title-block-banner-color":"white","title":"Threads, MPI and embarrassingly parallel","subtitle":"Making my job faster","author":"Prof. Peter Kille","date":"today","affiliation":"Cardiff University"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}