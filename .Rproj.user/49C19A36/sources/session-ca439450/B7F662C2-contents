---
title-block-style: default
title-block-banner: darkred
title-block-banner-color: white
title: "Slurm Dependencies"
subtitle: "Towards Pipelines"
author: "Prof. Peter Kille"
date: today
affiliation: "Cardiff University"
---

<body style="background-color:gray99;">

```{r include=FALSE}
knitr::opts_chunk$set(eval = FALSE)
```

![](Images/logo.jpg){width="10%"}

# Dependencies

If you have a series of bioinformatic processes you want to execute that for a 'chain' where each step is dependent on the output of the previous step this can be considered as a simple 'workflow'. You can code these very simply in slurm labeling each job with the same name ```#SBATCH --job-name=[my_unique_jobname]``` and then calling each slurm scrip from a single file with teh command ```sbatch -d singleton [script_name]```

##
Component scripts start:

```{bash}
#!/bin/bash
#SBATCH --job-name=test_workflow
#SBATCH --partition=defq       # the requested queue
#SBATCH --nodes=1              # number of nodes to use
#SBATCH --tasks-per-node=1     #
#SBATCH --cpus-per-task=1      #   
#SBATCH --mem-per-cpu=1000     # in megabytes, unless unit explicitly stated
#SBATCH --error=%J.err         # redirect stderr to this file
#SBATCH --output=%J.out        # redirect stdout to this file
......

and your workflow scripts 
#!/bin/bash

sbatch -d singleton [slurm_script1]

sbatch -d singleton [slurm_script2]
```

::: callout-important
## Exercise: Make your instance based transcriptomic processes slurm compatible WORKFLOW

In Session 5 of you bioinformatics introductory lectures you were given scripts to pre-process, map, count and call duplicates - [RNAseq Processing](https://peter-kille.github.io/Bioinformatics_TrainingWeb/5.1_RNAseq_processing.html).  I want you to transfer these and the data to scratch and convert these to slurm compatible scripts.

**LEAVE OUT STAR LIBRARY FORMTTING STEP**

:::
