[
  {
    "objectID": "Images/SRA-submission-master/Guide to NCBI SRA.html",
    "href": "Images/SRA-submission-master/Guide to NCBI SRA.html",
    "title": "Guide to contributing to NCBI SRA",
    "section": "",
    "text": "Are you looking to make your science more reproducible and transparent by making your data public? Or do you need to upload your data somewhere before your manuscript can be published? The NCBI Sequence Reach Archive (SRA) is a public searchable platform where genetic data and associated metadata may be uploaded, downloaded and reused.\n\n\nBefore beginning, it is helpful to read through several NCBI pages to understand the necessary data that is required for a successful upload. A good starting point is here. Understanding NCBI’s data hierarchy will also be helpful for understanding how your data should be submitted.\n####A summary of NCBI prefixes####\n\n\n\nPrefix\nAccession Name\nNCBI Definition\nExample\n\n\n\n\nPRJNA\nBioProject\nThe goal of your research effort\nPRJNA477007\n\n\nSRP\nStudy\nAn object that contains project metadata describing sequencing study or project\nSRP150953\n\n\nSAMN\nSample\nAn object that contains metadata describing the physical sample upon which a sequencing experiment was performed\nSAMN09463455\n\n\nSRX\nExperiment\nAn object containing metadata describing the library, platform selection and processing parameters involved in a sequencing experiment\nSRX7621456\n\n\nSRR\nRun\nAn object containing actual sequencing data for a sequencing experiment. Experiments may contain multiple runs if multiple sequencing instrument runs were needed, but preferable data structure is one run per experiment\nSRR10954732\n\n\n\n\n\n\nLog in on to the SRA Submission Portal Wizard\nCreate new submission by clicking on the ‘New Submission’ button. Your submission will receive a temporary SUB# ID, and you can use this to contact SRA staff if you have issues.\n#####Follow steps (you can leave at any step and return to it later). The Submission Portal will check to make sure everything is okay after each step and your position will be displayed on the progress bar.\n Figure 1. NCBI SRA submission progress bar\n1. Submitter information\nYou will be asked for your contact information and affiliations. You can also create a Group, which will allow your collaborators to read, modify, submit and delete your submissions.\n2. General information\nYou will be asked if you already registered your project and samples. Select yes or no. If yes, enter the accession number for your existing BioProject and you will be redirected to Step #6. If no, the Wizard will ask you to create them. Select the release date for your data. The default is immediately.\n3. Project information\nEnter information about your project, including title, a description and grant information.\n4. BioSample type\nSelect the best description for your data. A Pinsky Lab example would be ‘Model organism or animal sample’.\n5. BioSample attributes\nYou will be asked to provide information about your samples. You can either enter it directly into the built-in editor, or download a BioSamples template, fill it out and upload it.\nEnter each sample as a separate line and follow the directions on the template with regards to each of the colored data columns to ensure all necessary data have been included.\nA complete BioSamples template can be viewed here.\n6. SRA metadata\nIf you are entering metadata for new BioSamples, you may either enter the metadata using the built-in editor, or you may download a SRA metadata template, fill it out and upload it. Follow the instructions on the template to ensure all required information about sequencing methodology is included. An example of a SRA metadata template for new BioSamples may be viewed here.\nIf you have previously uploaded SRA metadata, download and complete the SRA metadata template. You must make sure that you include the BioSample accession number so that your new sequences are correctly linked to your existing BioSamples. An easy way to obtain these is to navigate to your previous submission within the submission portal and then to Download the attributes file with BioSample accessions (Figure 2).\n Figure 2. Where to download a file with BioSample accession numbers for previously submitted samples\n7. Files\nNCBI SRA accepts different file types. The Pinsky Lab aims to contribute FASTQ and BAM files for each sequenced individual, plus the reference that reads are aligned to. More information about file types may be found here. The file names need to be the same as those that you specified in the SRA Metadata.\nAs genomic data are large, there are several ways to transfer your sequence data to the NCBI SRA. Within this section of the Submission Portal, NCBI offers several ways to transfer your data: HTTP/Aspera Connect Plugin, FTP/Aspera Command Line or Amazon S3. If your data are less than 10 GB or you have fewer than 300 individuals, you can try to drag files directly into the Submission Portal, which will upload via HTTP. If your data are larger, select Request preload folder button. If you select the method you would like to use, some brief instructions will appear. Here, we will discuss various FTP options in more detail. Note that the time it takes to transfer your data will depend on the size of your data (often hours to days).\nFTP using the command line\n\nCreate a single directory with all the files you want to upload.\nEnsure you have FTP. You may need to install if not.\nNavigate to the directory all the files you want to upload are.\nEstablish a FTP connection by typing ftp -i.\nNext, type open ftp-private.ncbi.nlm.nih.gov.\nProvide your username this is listed in the Submission Portal, likely subftp.\nProvide the password listed in the Submission Portal.\nNavigate to your account folder listed in the Submission Portal: cd uploads/your_account_folder_name\nCreate a subfolder. You must do this or you will not be able to see your files in the preload option: mkdir new_folder_name\nNavigate into the target folder: cd new_folder_name\nCopy your files into the target folder: mput *\nGo back to the Submission Portal and select the folder to upload. It takes at least 10 minutes for transferred files to appear in the preload option.\nTo exit the FTP window, type bye.\n\nFTP using third-party software (Fetch, for example)\n\nOpen a Fetch window and connect to the NCBI server by typing in the hostname, your username and password, and direct Fetch to your new_folder_name within your_account_folder_name (Figure 3).\nOpen another Fetch window and connect to the location of your data.\nHighlight the files you want to transfer and drag into the Fetch window that is connected to the NCBI server. This will copy your files from your server to the NCBI server.\nGo back to the Submission Portal and select the folder to upload. It takes at least 10 minutes for transferred files to appear in the preload option.\n\n Figure 3. Screenshot showing how to connect to the NCBI server using Fetch\n8. Review & submit\nThe Submission Portal will check to make sure all your sequencing files have correctly transferred. It will also check to make sure that you have uploaded sequence data for all listed BioSamples. Review all the information before completing the submission. If an error arises during processing, you’ll receive an email asking you to contact SRA staff.\n\n\n\nIf you run into trouble, there is a SRA Submission Portal Troubleshooting Guide, or email SRA staff at sra@ncbi.nlm.nih.gov"
  },
  {
    "objectID": "Images/SRA-submission-master/Guide to NCBI SRA.html#getting-started",
    "href": "Images/SRA-submission-master/Guide to NCBI SRA.html#getting-started",
    "title": "Guide to contributing to NCBI SRA",
    "section": "",
    "text": "Before beginning, it is helpful to read through several NCBI pages to understand the necessary data that is required for a successful upload. A good starting point is here. Understanding NCBI’s data hierarchy will also be helpful for understanding how your data should be submitted.\n####A summary of NCBI prefixes####\n\n\n\nPrefix\nAccession Name\nNCBI Definition\nExample\n\n\n\n\nPRJNA\nBioProject\nThe goal of your research effort\nPRJNA477007\n\n\nSRP\nStudy\nAn object that contains project metadata describing sequencing study or project\nSRP150953\n\n\nSAMN\nSample\nAn object that contains metadata describing the physical sample upon which a sequencing experiment was performed\nSAMN09463455\n\n\nSRX\nExperiment\nAn object containing metadata describing the library, platform selection and processing parameters involved in a sequencing experiment\nSRX7621456\n\n\nSRR\nRun\nAn object containing actual sequencing data for a sequencing experiment. Experiments may contain multiple runs if multiple sequencing instrument runs were needed, but preferable data structure is one run per experiment\nSRR10954732"
  },
  {
    "objectID": "Images/SRA-submission-master/Guide to NCBI SRA.html#submitting",
    "href": "Images/SRA-submission-master/Guide to NCBI SRA.html#submitting",
    "title": "Guide to contributing to NCBI SRA",
    "section": "",
    "text": "Log in on to the SRA Submission Portal Wizard\nCreate new submission by clicking on the ‘New Submission’ button. Your submission will receive a temporary SUB# ID, and you can use this to contact SRA staff if you have issues.\n#####Follow steps (you can leave at any step and return to it later). The Submission Portal will check to make sure everything is okay after each step and your position will be displayed on the progress bar.\n Figure 1. NCBI SRA submission progress bar\n1. Submitter information\nYou will be asked for your contact information and affiliations. You can also create a Group, which will allow your collaborators to read, modify, submit and delete your submissions.\n2. General information\nYou will be asked if you already registered your project and samples. Select yes or no. If yes, enter the accession number for your existing BioProject and you will be redirected to Step #6. If no, the Wizard will ask you to create them. Select the release date for your data. The default is immediately.\n3. Project information\nEnter information about your project, including title, a description and grant information.\n4. BioSample type\nSelect the best description for your data. A Pinsky Lab example would be ‘Model organism or animal sample’.\n5. BioSample attributes\nYou will be asked to provide information about your samples. You can either enter it directly into the built-in editor, or download a BioSamples template, fill it out and upload it.\nEnter each sample as a separate line and follow the directions on the template with regards to each of the colored data columns to ensure all necessary data have been included.\nA complete BioSamples template can be viewed here.\n6. SRA metadata\nIf you are entering metadata for new BioSamples, you may either enter the metadata using the built-in editor, or you may download a SRA metadata template, fill it out and upload it. Follow the instructions on the template to ensure all required information about sequencing methodology is included. An example of a SRA metadata template for new BioSamples may be viewed here.\nIf you have previously uploaded SRA metadata, download and complete the SRA metadata template. You must make sure that you include the BioSample accession number so that your new sequences are correctly linked to your existing BioSamples. An easy way to obtain these is to navigate to your previous submission within the submission portal and then to Download the attributes file with BioSample accessions (Figure 2).\n Figure 2. Where to download a file with BioSample accession numbers for previously submitted samples\n7. Files\nNCBI SRA accepts different file types. The Pinsky Lab aims to contribute FASTQ and BAM files for each sequenced individual, plus the reference that reads are aligned to. More information about file types may be found here. The file names need to be the same as those that you specified in the SRA Metadata.\nAs genomic data are large, there are several ways to transfer your sequence data to the NCBI SRA. Within this section of the Submission Portal, NCBI offers several ways to transfer your data: HTTP/Aspera Connect Plugin, FTP/Aspera Command Line or Amazon S3. If your data are less than 10 GB or you have fewer than 300 individuals, you can try to drag files directly into the Submission Portal, which will upload via HTTP. If your data are larger, select Request preload folder button. If you select the method you would like to use, some brief instructions will appear. Here, we will discuss various FTP options in more detail. Note that the time it takes to transfer your data will depend on the size of your data (often hours to days).\nFTP using the command line\n\nCreate a single directory with all the files you want to upload.\nEnsure you have FTP. You may need to install if not.\nNavigate to the directory all the files you want to upload are.\nEstablish a FTP connection by typing ftp -i.\nNext, type open ftp-private.ncbi.nlm.nih.gov.\nProvide your username this is listed in the Submission Portal, likely subftp.\nProvide the password listed in the Submission Portal.\nNavigate to your account folder listed in the Submission Portal: cd uploads/your_account_folder_name\nCreate a subfolder. You must do this or you will not be able to see your files in the preload option: mkdir new_folder_name\nNavigate into the target folder: cd new_folder_name\nCopy your files into the target folder: mput *\nGo back to the Submission Portal and select the folder to upload. It takes at least 10 minutes for transferred files to appear in the preload option.\nTo exit the FTP window, type bye.\n\nFTP using third-party software (Fetch, for example)\n\nOpen a Fetch window and connect to the NCBI server by typing in the hostname, your username and password, and direct Fetch to your new_folder_name within your_account_folder_name (Figure 3).\nOpen another Fetch window and connect to the location of your data.\nHighlight the files you want to transfer and drag into the Fetch window that is connected to the NCBI server. This will copy your files from your server to the NCBI server.\nGo back to the Submission Portal and select the folder to upload. It takes at least 10 minutes for transferred files to appear in the preload option.\n\n Figure 3. Screenshot showing how to connect to the NCBI server using Fetch\n8. Review & submit\nThe Submission Portal will check to make sure all your sequencing files have correctly transferred. It will also check to make sure that you have uploaded sequence data for all listed BioSamples. Review all the information before completing the submission. If an error arises during processing, you’ll receive an email asking you to contact SRA staff."
  },
  {
    "objectID": "Images/SRA-submission-master/Guide to NCBI SRA.html#troubleshooting",
    "href": "Images/SRA-submission-master/Guide to NCBI SRA.html#troubleshooting",
    "title": "Guide to contributing to NCBI SRA",
    "section": "",
    "text": "If you run into trouble, there is a SRA Submission Portal Troubleshooting Guide, or email SRA staff at sra@ncbi.nlm.nih.gov"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1"
  },
  {
    "objectID": "3.2_Containers.html#linux-containers---from-linux-primitives-to-container-run-time-software",
    "href": "3.2_Containers.html#linux-containers---from-linux-primitives-to-container-run-time-software",
    "title": "Continers",
    "section": "Linux Containers - from Linux primitives to container run-time software",
    "text": "Linux Containers - from Linux primitives to container run-time software\nLinux containers are a method of isolating running process from the host system. Containers will provide:\n\nfilesystem isolation\nprocess isolation\nresource limitations, and\nnetwork isolation (not discussed in this tutorial)\n\nWe will discover exactly what these mean as we progress through the tutorial.",
    "crumbs": [
      "Session 3: FAIR Data",
      "Continers"
    ]
  },
  {
    "objectID": "3.2_Containers.html#containers-vs-virtual-machines",
    "href": "3.2_Containers.html#containers-vs-virtual-machines",
    "title": "Continers",
    "section": "containers vs virtual machines",
    "text": "containers vs virtual machines\nAny introductory course on containers will almost always start with a comparison of containers vs. virtual machines. (I am unsure how useful this actually is to students undertaking their first course on Linux, they may well meet containers before even coming across Virtual Machines). Assuming you’re aware of both, we provide a brief comparison now:\nIn effect, virtual machines virtualise hardware, and containers will virtualise the Linux kernel. Virtual machines will run their own Linux kernel on top of virtualised hardware; containers will share the host’s kernel. This means any processes in a virtual machine will often need to pass through two separate kernels to perform a given task; conversely, any interaction with hardware from processes within a container, need only deal with the single host kernel. This often means that container processes have little overhead, and processes are handled at host-native speeds. Whereas virtual machine processes have more overhead and will often suffer in performance.",
    "crumbs": [
      "Session 3: FAIR Data",
      "Continers"
    ]
  },
  {
    "objectID": "3.2_Containers.html#container-runtimes-dockersingularitypodman",
    "href": "3.2_Containers.html#container-runtimes-dockersingularitypodman",
    "title": "Continers",
    "section": "Container Runtimes – docker/singularity/podman",
    "text": "Container Runtimes – docker/singularity/podman\nThe above tutorial built up a container using linux primitives alone. This is obviously a rather cumbersome approach to creating containers and container images. The following set of tools are some of the, perhaps, more well-known container runtime engines and container image and management software.\n\ndocker*\npodman*\ncontainerd\nsystemd-nspawn\ncharliecloud*\nsingularity*\n\nThese container runtime engines are (mostly) command-line tools that make creating, updating, packaging, distributing, and running containers significantly easier than using the Linux primitives we met earlier. This allowed them to become very popular with system administrators, and also program developers to distribute their software. These tools, at their core, do exactly what we did with the Linux primitives, but abstract many of the complexities away from managing cgroups, namespaces, and chroot, but in a more convenient way.\nThose container run-time engines marked with the ‘*’, above, are the container run-time engines that computational scientists are most likely to come across. Docker/Podman have very similar features to one another, and are best suited to personal desktops/laptops (i.e. single tenancy systems). Singularity/Charliecloud are also very similar to one another, and will often be available to researchers on multi-tenancy HPC clusters.\nThe reason that Docker/Podman** are not suitable to multi-tenancy systems is that by default containers run as the root user. If you launch a container with docker, unless you specify a specific user, you’re going to run that container as the root user. And this is the same root user inside the container as the root on the host (much the same as in our Linux primitives tutorial above). In addition, when providing a non-privileged user access to the docker runtime, that user can easily get access to the host’s root account with just a few Docker commands - an obvious security risk in multi-tenancy environments.\n** Podman in fact does have some mechanism to not run as the host’s root inside the container, but it’s still not often found on multi-tenancy servers.\nConversely, the container run-time engines of Singularity and Charliecloud are safe to run in multi-tenancy environments. These run-time engines use a more recent kernel feature of User Namespaces. These are like the namespaces we met in the above chroot tutorial, except that a “root” user inside the container is mapped to a non-privileged user on the host. This is all handled by the Linux kernel - so no new security boundary exists (we’ve already accepted the Linux kernel as safe).",
    "crumbs": [
      "Session 3: FAIR Data",
      "Continers"
    ]
  },
  {
    "objectID": "3.2_Containers.html#container-repositories",
    "href": "3.2_Containers.html#container-repositories",
    "title": "Continers",
    "section": "Container repositories",
    "text": "Container repositories\n\n(Docker)[https://hub.docker.com/]\n(Singularity)[sylabs.io]\nPodman – Search within application - podman search \n(Charliecloud)[https://github.com/hpc/charliecloud]\n\n\nUse Docker hub to identify a containers that would support fastqc, trimmomatic (or fastp) and multiqc.\n\nHow would you judge a container is valid and ‘safe’ to use\nCan you identify a container that has all the of the programs",
    "crumbs": [
      "Session 3: FAIR Data",
      "Continers"
    ]
  },
  {
    "objectID": "3.2_Containers.html#pulling-and-using-a-container-in-an-interactive-job",
    "href": "3.2_Containers.html#pulling-and-using-a-container-in-an-interactive-job",
    "title": "Continers",
    "section": "Pulling and using a Container in an interactive job",
    "text": "Pulling and using a Container in an interactive job\nRunning interactive Singularity containers is simple:\n\nPull image (or use an existing one)\nCreate and enter an interactive slurm job\nRun and enter the Singularity container\n\n(The final two steps could also be combined into one command.)",
    "crumbs": [
      "Session 3: FAIR Data",
      "Continers"
    ]
  },
  {
    "objectID": "3.2_Containers.html#example-of-pulling-down-and-running-an-ubuntu-image",
    "href": "3.2_Containers.html#example-of-pulling-down-and-running-an-ubuntu-image",
    "title": "Continers",
    "section": "Example of pulling down and running an Ubuntu image",
    "text": "Example of pulling down and running an Ubuntu image\n\n## Enter interactive node on defq\nsrun -c 4 --mem=8G -p defq --pty bash\n\n## Load latest singularity module\nmodule load apptainer/1.3.4\n\n##make and enter a directory for your containers within your mydata directory\ncd ~/scratch\nmkdir singularity\ncd singularity/\n\n##Create cache and point the singularity cache directory\nmkdir cache\nexport APPTAINER_TMPDIR=~/scratch/singularity/cache/\nexport APPTAINER_CACHEDIR=~/scratch/singularity/cache/\n\n## Create and enter a working folder\nmkdir working\ncd working/\n\n\n## Pull ubuntu version 20.04 from Docker Hub and store as .sif image\nsingularity pull ubuntu.sif docker://ubuntu:20.04\n\n# Run singularity container\n# --contain is optional but will ensure $HOME is not auto mounted\nsingularity run --contain ubuntu.sif\n\n# You should now find yourself within the container\nSingularity&gt;\n\n# Try checking the operating system by running\ncat /etc/os-release\n# If working this should be Ubuntu if not working then RockyLinux (iago OS)\n\n# To exit container\nexit\n\n\nSecurity warning\nThe --contain option when running a container is considered optional in singularity but essential by the BiocomputingHub. By default Singularity will try to auto-mount your $HOME folder into the container. While this may not seem like an issue, if you are pulling down an unknown Docker Hub image (or using a .sif file created by someone else) this potentially could be compromised in some form. For example if it has a runscript to delete all files and folders on the mounted home drive then it could potentially wipe your home drive (or anything else you have mounted). Singularity runs with the same permission inside the container as outside the container for security, no critical protected files will be removed if you do not have permission to delete them.\n\n\nWhat next ?\nThe ubuntu image is pretty useless by itself but serves as a good demo, there are however multiple pre-made images on Docker Hub that can be downloaded and used in Singularity (https://hub.docker.com/). Something more useful would be an up to date python image, such as python:3.9.12-buster\nFollow the previous guide up to the step where you pull down the image…\n\n# Pull Python version 3.9.12 from Docker Hub and store as .sif image\nsingularity pull python3.9.12.sif docker://python:3.9.12-buster\n\n# Run the container in singularity\nsingularity run --contain python3.9.12.sif\n\nPython 3.9.12 (main, Apr 20 2022, 18:47:18) \n[GCC 8.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; \n\nYou will notice that unlike the ubuntu image this launches straight into the python application. This is caused by the singularity runscript that is set within the container. To check what the runscript will do on run, you can run this command outside of the container :\n\nsingularity inspect -r python3.9.12.sif\n\n# At the top of the resulting text you will see something like:\n#!/bin/sh\nOCI_ENTRYPOINT=''\nOCI_CMD='\"python3\"'\nCMDLINE_ARGS=\"\"\n\nThe OCI_CMD is the command running inside the container on launch, for the python container this is launching python3. If you want to launch the container without auto launching python3 (without the runscript) then you can use singularity shell, using the python container as an example:\n\nsingularity shell --contain python3.9.12.sif \nSingularity&gt; \n\n\n\nRunning scripts and binding folders to container\nAs well as run and shell the other most useful command is exec, this allows you to execute a command when the container is launched.\nFor example to run a simple Python script.\nMake a demo python script called example.py with the following contents.\n\n#!/usr/bin/python3\nprint('This is a demo python script')\n\nMake it executable\n\nchmod +x example.py\n\nTo be able to run the example.py script within the container you have to make the folder location accessible within the container. The easiest way to do this is to bind (mount/attach) the folder to a specified location within the container using the -B option.\nAssume that example.py is located at /mnt/clusters/sponsa/data/$USER/script\n\nsingularity exec --contain -B /mnt/clusters/sponsa/data/$USER/script:/mnt/clusters/sponsa/data/$USER/script python3.9.12.sif \\\npython /mnt/clusters/sponsa/data/$USER/script/example.py\n&gt; This is a demo python script\n\nThis command is mounting the /mnt/clusters/sponsa/data/$USER/script folder to the same location within the python.sif container, then executing the python command with the example.py script.\nThe output is passed back outside the container and the container is automatically killed as it has no other tasks to perform.\nYou do not need to bind to the same location as the folder (/mnt/clusters/sponsa/data/$USER/script…) but it’s pretty confusing if you don’t !!\n\nsingularity exec --contain -B /mnt/clusters/sponsa/data/$USER/script:/bind python3.9.12.sif python /bind/example.py\n&gt; This is a demo python script",
    "crumbs": [
      "Session 3: FAIR Data",
      "Continers"
    ]
  },
  {
    "objectID": "3.2_Containers.html#using-containers-in-slurm-scripts",
    "href": "3.2_Containers.html#using-containers-in-slurm-scripts",
    "title": "Continers",
    "section": "Using containers in Slurm scripts",
    "text": "Using containers in Slurm scripts\nThis is a workflow of how to safely pull and convert an existing docker image from Docker Hub. Docker Hub is a cloud based repository used mainly for storing and distributing container images.\nOne of the benefits of Singularity is the ability to convert Docker images into Singularity images.\nFor this workflow we will be pulling and converting a container image for fastqc, this is a popular program used for quality assesses NGS data.\nNot all Docker container images will work in Singularity, Docker requires root access to run.Singularity on gomphus runs without any root access so the container images are immutable. \n\nSecurity : Finding a safe image to download\nAny docker user can upload to Docker Hub, please do not automatically assume all of these images are safe and working correctly !\nTo find a safe image to download then treat Docker Hub the same as downloading software from any internet site.\nSearching for fastqc on Docker Hub (https://hub.docker.com/search?q=fastqc) brings up lots of container images. Look for those images from reliable sources (such as the application creator), updated relatively recently, with a large number of downloads and if possible stars (these are given by previous users).\n(Often official image are listed on the github sites of the software - this is not the case for fastqc as it is predates container use)\n ### Pulling the image\nRun the usual setup procedure to create a singularity environment, it’s important to set the cachedir as some container images will be bigger than your %HOME space.\n\n# Enter interactive node on defq (epyc could also be used)\nsrun -c 4 --mem=8G -p defq --pty bash\n\n# Load latest singularity module\nmodule load singularity/3.8.7\n\n# Point the singularity cache directory\nexport SINGULARITY_TMPDIR=/mnt/clusters/sponsa/data/$USER/singularity/cache/\nexport SINGULARITY_CACHEDIR=/mnt/clusters/sponsa/data/$USER/singularity/cache\n\n# Enter working folder\ncd /mnt/clusters/sponsa/data/$USER/singularity/\n\nPull the container image with the following command. To ensure reproducibly in some form it’s better to use the tag specifying the version number in the pull command. Also name your sif file with a similar tag for future reference.\n\nsingularity pull fastqc.sif docker://staphb/fastqc\n\nThis will start downloading the image layers and converting to .sif. The process can take a while so be patient, when nearly finished you will see ‘Creating SIF file…’\n\nINFO:    Converting OCI blobs to SIF format\nINFO:    Starting build...\nGetting image source signatures\nCopying blob 7595c8c21622 done  \nCopying blob d13af8ca898f done  \nCopying blob 70799171ddba done\n.\n.\n.\nINFO:    Creating SIF file...\n\n\n\n\nUsing the sif image\nDo not run the sif image, you will not be able to find out what it does until it’s too late,start by inspecting the runscript (what it will do when run) inside the image using:\n\nsingularity inspect -r fastqc.sif\n\nThis shows the runscript as running OCI_CMD=‘“/bin/bash”’, so will simply run a bash prompt inside the container which is a good start.\nNow try entering the container, I would advise ignoring the runscript option and using ‘singularity shell’ to open a shell inside the container. Remember to also use –contain, this will isolate the container and prevent it from mounting your $HOME folder which would be a big security risk on unknown containers.\n\nsingularity shell --contain fastqc.sif\n\nThis should put you inside the container with a Singularity&gt; prompt. Run some commands to verify the container is what you expect it to be. For example checking for operating system version and in this case checking the Trinity version matches what was specified on the docker tags. Exit the container when finished.\n\n# OS version\nSingularity&gt; cat /etc/os-release \nNAME=\"Ubuntu\"\nVERSION=\"16.04.7 LTS (Xenial Xerus)\"\nID=ubuntu\nID_LIKE=debian\nPRETTY_NAME=\"Ubuntu 16.04.7 LTS\"\nVERSION_ID=\"16.04\"\nHOME_URL=\"http://www.ubuntu.com/\"\nSUPPORT_URL=\"http://help.ubuntu.com/\"\nBUG_REPORT_URL=\"http://bugs.launchpad.net/ubuntu/\"\nVERSION_CODENAME=xenial\nUBUNTU_CODENAME=xenial\n\n# Trinity version\nSingularity&gt; fastqc -v\nperl: warning: Setting locale failed.\nperl: warning: Please check that your locale settings:\n        LANGUAGE = (unset),\n        LC_ALL = (unset),\n        LANG = \"en_GB.UTF-8\"\n    are supported and installed on your system.\nperl: warning: Falling back to the standard locale (\"C\").\nFastQC v0.11.9\n\n\n# exit container\nSingularity&gt; exit\n\n\n\n\nWhere to store the container\nOnce verified you can move the fastqc.sif anywhere you need. If storing the containers for any period of time just remember that any security issues or bugs will also remain in the container, they are essentially locked in time.\n\n\n\nReproducibility\nIt’s important to document how the container was pulled or built, that way it can be re-created or updated if required again in the future. As a researcher you will also want to aid reproducibility of your workflow by at least recording any relevant versions of code or container-images. So you may want to rename this container with its version.\n\nmv fastqc.sif fastqcv0.11.9.sif\n\n\n\n\nInteractive mode\nLet’s use the fastqc.sif image to run interactively, including binding a directory to run some tests. We will be using sample_data from Session5 RNAseq-Processing /mnt/clusters/sponsa/data/classdata/Bioinformatics/Session5/RNAseq-Processing/fastq/SRR*.fastq\nCreate an interactive node on the defq partition.\n\nsrun -c 4 --mem=8G -p defq --pty bash\n\n# load singularity module\nmodule load singularity/3.8.7\n\nmkdir /mnt/clusters/sponsa/data/$USER/singularity/test_data/\n\ncd /mnt/clusters/sponsa/data/$USER/singularity/test_data/\n\ncp /mnt/clusters/sponsa/data/classdata/Bioinformatics/Session5/RNAseq-Processing/fastq/SRR*.fastq .\n\nTo use the test data we need to make that folder accessible inside the container. We could bind the complete /mnt/clusters/sponsa/data/$USER/ folder but it’s more secure to only bind the folder(s) you require inside the container. You can specify where the folder is mounted inside the container but I will use the same folder location as outside the container to make it easier to remember.\nAs we are using –contain to protect the home folder it’s a good idea to change to a working directory where you have full write access. Using –pwd is a simple way of setting the default folder on container launch \n\n# use --bind or -B to bind folder\n# use --contain to prevent mounting of home folder\nsingularity shell --contain --bind /mnt/clusters/sponsa/data/$USER/singularity/test_data/:/mnt/clusters/sponsa/data/$USER/singularity/test_data/ --pwd \\\n/mnt/clusters/sponsa/data/$USER/singularity/test_data/  /mnt/clusters/sponsa/data/${USER}/singularity/working/fastqcv0.11.9.sif\n\n# You should now see the singularity&gt; prompt inside the container\n\n# test if sample data is attached correctly\nsingularity&gt; ls /mnt/clusters/sponsa/data/$USER/singularity/test_data/\n\nOnce inside the container you can run standard trinity commands, you may need to specify output locations. If the output tries to write to an area outside of your mounted/bound folder it will either fail due to lack of permissions (images are read only) or it will appear to write but dissapear when the container is exited.\n\n# run a standard fastqc command within container\nfastqc -t 4 SRR*\n\n\n\n\nSlurm\nOnce confident that the container image does the job you require you can wrap it into a slurm script to automate the process.\n\n#!/bin/bash\n#SBATCH --partition=defq       # the requested queue\n#SBATCH --nodes=1              # number of nodes to use\n#SBATCH --tasks-per-node=1     #\n#SBATCH --cpus-per-task=4      #   \n#SBATCH --mem=8G     # in megabytes, unless unit explicitly stated\n#SBATCH --error=%J.err         # redirect stderr to this file\n#SBATCH --output=%J.out        # redirect stdout to this file\n##SBATCH --mail-user=[insert email address]@Cardiff.ac.uk  # email address used for event notification\n##SBATCH --mail-type=all                                   # email on job start, failure and end\n\n\necho \"Some Usable Environment Variables:\"\necho \"=================================\"\necho \"hostname=$(hostname)\"\necho \\$SLURM_JOB_ID=${SLURM_JOB_ID}\necho \\$SLURM_NTASKS=${SLURM_NTASKS}\necho \\$SLURM_NTASKS_PER_NODE=${SLURM_NTASKS_PER_NODE}\necho \\$SLURM_CPUS_PER_TASK=${SLURM_CPUS_PER_TASK}\necho \\$SLURM_JOB_CPUS_PER_NODE=${SLURM_JOB_CPUS_PER_NODE}\necho \\$SLURM_MEM_PER_CPU=${SLURM_MEM_PER_CPU}\n\n# Write jobscript to output file (good for reproducability)\ncat $0\n\nLoad the singularity module and then set the location of the .sif container image. The TOTAL_RAM variable simply converts ${SLURM_MEM_PER_NODE} back into GB, trinity can only use GB and slurm tends to convert GB to MB. WORKINGFOLDER in this case is the location of the test data and what we will also use as default folder within the container, as it will be set to our external bound folder it will have full read&write access. The BINDS variable will container any folders you wish to bind into the container, you can specify multiple folders seperated by ‘,’. You need to specify the iago location of the folder and then the mount point within the container.\n\n# load singularity module\nmodule load singularity/3.8.7\n\n# set singularity image\nSINGIMAGEDIR=/mnt/clusters/sponsa/data/${USER}/singularity/working/\nSINGIMAGENAME=fastqcv0.11.9.sif\n\n# Set working directory \nWORKINGFOLDER=/mnt/clusters/sponsa/data/$USER/singularity/test_data/\n\n# set folders to bind into container\nexport BINDS=\"${BINDS},${WORKINGFOLDER}:${WORKINGFOLDER}\"\n\nTo be able to run a Trinity script inside the container the script needs to be in a location that is accessible within the container, the easiest folder in this case is the WORKINGFOLDER. You can either generate a bash script (trinity_source_commands.sh) as part of the sbatch script or link to an existing script. Adding the commands to the sbatch script is better for reproducability, having a seperate script is more likely to be forgotten in a few months but the choice is yours.\nThe Trinity commands are pretty standard, but we make use of the WORKINGFOLDER variable and slurm environment variables to declare RAM and CPU. I like to echo the TOTAL_RAM AND CPU to the output file (JOBID.out) to check the conversion worked correctly and the CPU total is also matching, obviously not essential.\n\n############# SOURCE COMMANDS ##################################\ncat &gt;${WORKINGFOLDER}/fastqc_source_commands.sh &lt;&lt;EOF\nfastqc -t ${SLURM_CPUS_PER_TASK} SRR*\n\necho TOTAL_RAM=${TOTAL_RAM}\necho CPU=${SLURM_CPUS_PER_TASK}\n\nEOF\n################ END OF SOURCE COMMANDS ######################\n\nThis is the magic command that sets up the singularity container environment, binds requested drives, sets the working directory and then executes the bash script inside of the container.\n\nsingularity exec --contain --bind ${BINDS} --pwd ${WORKINGFOLDER} ${SINGIMAGEDIR}/${SINGIMAGENAME} bash ${WORKINGFOLDER}/fastqc_source_commands.sh\n\nThis is the same as all slurm jobs where a JOBID will be created and queued within the slurm queue, on completion of the script the container and job will close. The slurm logs .out and .err will contain the job details so please check on completion.",
    "crumbs": [
      "Session 3: FAIR Data",
      "Continers"
    ]
  },
  {
    "objectID": "3.2_Containers.html#linux-chroot-jails",
    "href": "3.2_Containers.html#linux-chroot-jails",
    "title": "Continers",
    "section": "Linux chroot Jails",
    "text": "Linux chroot Jails\nJails are similar to containers (although cannot quite be classed as a container because by themselves they do not fit the isolation criteria). Nonetheless, we will start our journey with jails, because all the tools needed to create jails are usually already available by default on any Linux system.\nJails have been available to Linux since the 1980’s (long before containers were introduced to the Linux kernel, around 2006). Jails provide a method of isolating the filesystem, so that the jail cannot see the host’s filesystem. During this tutorial we will further implement limitations on the jail, isolating various other aspects of the host system away from the jail, to the point where we convert our jail to a full-fledged container.\nFirst of all, we want to virtualise a linux system, so let’s start by building up a typical linux filesystem - like the one listed above. Download the Alpine minirootfs and unpack. Alpine is a well-known and (very) lightweight linux distribution.\n\nmkdir alpine && curl -L https://dl-cdn.alpinelinux.org/alpine/v3.16/releases/x86_64/alpine-minirootfs-3.16.2-x86_64.tar.gz   | tar xvzf - -C alpine\n#\n# for the purposes of this tutorial only: add a file to alpine, to help with navigation when switching between host and jail.\ntouch alpine/in_alpine\n#\n# change ownership to the alpine folder to root (recursively)\nsudo chown -R root:root alpine/\n\nTake a look at the resulting alpine folder, and you should see something similar to the filesystem tree shown above.",
    "crumbs": [
      "Session 3: FAIR Data",
      "Continers"
    ]
  },
  {
    "objectID": "3.2_Containers.html#filesystem-isolation",
    "href": "3.2_Containers.html#filesystem-isolation",
    "title": "Continers",
    "section": "filesystem isolation",
    "text": "filesystem isolation\nCreate and enter a chroot jail. The following command will create a jail, and run the /bin/sh process within that jail. You can exit the chroot jail whenever you like by typing exit or ctrl-d.\n\nsudo chroot alpine /bin/sh -l\n\nThis jail provides us with filesystem isolation. Look around the filesystem within the jail, and you will see no way out to the host filesystem.",
    "crumbs": [
      "Session 3: FAIR Data",
      "Continers"
    ]
  },
  {
    "objectID": "3.2_Containers.html#process-isolation",
    "href": "3.2_Containers.html#process-isolation",
    "title": "Continers",
    "section": "process isolation",
    "text": "process isolation\nHowever, this chroot jail does nothing about process-isolation, network-isolation, or resource-limitation. In fact, probing for any information about any processes currently fails (even processes within the chroot jail itself). This is because the /proc, /sys, and /dev pseudo-filesystems have not yet been made available to the chroot jail, so therefore the jail has no route to probe the linux kernel. Let’s fix that now, and mount the pseudo-filesystems.\n\n# we're still in the jail\nmount -t proc none /proc\nmount -t sysfs none /sys\nmount -t devtmpfs none /dev\n\nNow we can prove that no process-isolation takes place. Because the chroot jail now has access to the host kernel (because of our mounted /proc, /sys, and /dev), we can now probe the running processes from inside the chroot jail. Running\n\ntop\n# press 'q' to exit when ready to do so\n\nyou can see not only the running processes inside the chroot jail, but all processes running on the host system. i.e. there is no process isolation within this chroot jail.",
    "crumbs": [
      "Session 3: FAIR Data",
      "Continers"
    ]
  },
  {
    "objectID": "3.2_Containers.html#resource-limitation",
    "href": "3.2_Containers.html#resource-limitation",
    "title": "Continers",
    "section": "resource limitation",
    "text": "resource limitation\nTo test our chrooot jails for resource limitation, we will create a chroot jail (with the mounted kernel folders), and install some stress-test software to see if there is any limitation to how many resources we can consume (answer: resources are not restrained in any way).\n\nsudo chroot alpine /bin/sh -l\n#\n# provide the chroot with access to the google dns service\necho 'nameserver 8.8.8.8 &gt;/etc/resolv.conf\n#\n# install some software (apk is the alpine package manager)\napk add python3 python3-dev gcc linux-headers musl-dev bash\n\nNow that some useful software is installed into our chroot image, exit and re-enter the chroot jail, this time with a slightly more useful shell.\n\n# ctrl-d to exit the chroot jail, then re-enter\nsudo chroot alpine /bin/bash -l\n\nNow we’re back inside the chroot jail, let’s install our stress-test software:\n\npython3 -m venv /opt/pycont --clear --copies\n. /opt/pycont/bin/activate\npip install stress\n\nNow let’s stress out our chroot jail.\n\n# this commands will run $THREADS threads.\nTHREADS=16\nstress -c $THREADS\n\nYou can check out the cpu usage from a terminal on the host machine. You can choose any number of threads, you will find the only cpu-limit is that of the hardware of your host machine.\nSo, we have seen that chroot jails will isolate a filesystem, but does not limit resources, or isolate processes.",
    "crumbs": [
      "Session 3: FAIR Data",
      "Continers"
    ]
  },
  {
    "objectID": "3.2_Containers.html#limiting-resources-with-cgroups",
    "href": "3.2_Containers.html#limiting-resources-with-cgroups",
    "title": "Continers",
    "section": "Limiting resources with cgroups",
    "text": "Limiting resources with cgroups\nIn order to limit processes to a certain amount of resource (CPU, RAM), one can use cgroups (Control Groups). cgroups is a Linux kernel feature that will allow the limitation of resources (CPU, RAM) available to a process. And because UNIX processes are hierarchical, children of the cgroup’d process are in the same cgroup and will contribute to the same limit.\nLet’s set up a cgroup and attach our chroot jail to it.\n\nsudo cgcreate -a $USER -g memory,cpuset:alpine-jail\n\nThis will set up a cgroup within the linux kernel, called alpine-jail, owned by the $USER that ran the command. We can see the setup within the kernel /sys folder:\nls -l /sys/fs/cgroup/*/alpine-jail  #* list the contents of multiple alpine-jail folders \nLimit the resources available to this cgroup by making changes to the kernel data-structures:\n\n# Note, since the user $USER owns this data-structure, sudo access is not necessary\n# Limit RAM to 4GB\necho 2000000000 &gt; /sys/fs/cgroup/memory/alpine-jail/memory.limit_in_bytes\n# Limit the cgroup to use the first 5 CPUs only\necho 0-4 &gt; /sys/fs/cgroup/cpuset/alpine-jail/cpuset.cpus\n\nNow we can create the chroot jail, this time running under our newly created cgroup.\n\nsudo cgexec -g memory,cpuset:alpine-jail  chroot alpine /bin/bash -l\n\nWhile within the chroot jail, run the stress command and check out top from a host terminal.\n\n. opt/pycont/bin/activate\nstress -c 10\n\nBecause we limited the cgroup to cpus 0-4 only, our chroot jail only has 5 cpus from the host available to it. By running stress across 10 cpus, you should see that the process is limited, and 10 threads are being shared across 5 cpus, each thread running at approximately 50% cpu usage.\nOur chroot jail can now almost be classed as a container. All that is left is for us to do is isolate processes.",
    "crumbs": [
      "Session 3: FAIR Data",
      "Continers"
    ]
  },
  {
    "objectID": "3.2_Containers.html#isolating-processes-with-namespaces",
    "href": "3.2_Containers.html#isolating-processes-with-namespaces",
    "title": "Continers",
    "section": "Isolating processes with Namespaces",
    "text": "Isolating processes with Namespaces\nOur next target for creating a full-fledged container, is to limit our virtual linux environment to have access to list and probe it’s own processes only. That is, the jail should not be able to see any information about any processes that were not spawned within that jail. Unsurprisingly, the Linux kernel has a feature to do exactly that. These are called Namespaces. Namespaces allow us to hide processes from other processes. There are many different types of Namespaces (process, network, mount, and more), and their description are beyond the scope of this tutorial. But for our purposes, we will simply bundle up the relevant Namespaces into a single command. And the command we will now introduce, is called unshare.\n\nsudo unshare  --mount --uts --ipc --pid --fork --user --map-root-user chroot alpine /bin/bash\n\nThe above unshare command will create a new namespace for the chroot alpine jail. This namespace is a new environment that’s isolated on the system with its own processes (PIDs), and mounts (volumes) etc.\nBy running the above Namespaced jail, we immediately enter the isolated environment. And since we have restricted mounts (check out the unshare arguments), we are not able to mount all the psuedo-filesystems as we did previously. However, just mounting the /proc filesystem will suffice:\n\nmount -t proc none /proc\n\nNow, running the top command, we can see we have isolated processes - the only processes listed are those that have been spawned from within the jail/container. That is, we are not able to see any information about processes on the host.\n\ntop\n\nWe have now used the kernel feature Namespaces to protect the host processes from the container processes.\nNext, we will collate what we have learned, and form a command that will create a truly isolated container runtime, using linux primitives and kernel features alone.",
    "crumbs": [
      "Session 3: FAIR Data",
      "Continers"
    ]
  },
  {
    "objectID": "3.2_Containers.html#putting-it-all-together",
    "href": "3.2_Containers.html#putting-it-all-together",
    "title": "Continers",
    "section": "Putting it all together",
    "text": "Putting it all together\nHaving run through all our isolation techniques, the following command will create a container from using Linux primitives only.\n\n# assumes the cgroup has been cerated\nsudo  cgexec -g memory,cpuset:alpine-jail  \\\n  unshare  --mount --uts --ipc --pid --fork --user --map-root-user \\\n  chroot alpine /bin/bash\n# you may need to mount the /proc filesystem again\nmount -t proc none /proc\n\nThe command may look a little complicated, but we have met all the components of this command separately throughout this tutorial: the chroot will isolate a filesystem; the unshare will isolate the processes; and the cgexec will limit the resources.",
    "crumbs": [
      "Session 3: FAIR Data",
      "Continers"
    ]
  },
  {
    "objectID": "3.2_Containers.html#cleaning-up",
    "href": "3.2_Containers.html#cleaning-up",
    "title": "Continers",
    "section": "Cleaning up",
    "text": "Cleaning up\nTo clean up the chroot jail, exit the jail, then unmount the pseudo-filesystems..\n\nbash sudo umount alpine/proc alpine/sys alpine/dev",
    "crumbs": [
      "Session 3: FAIR Data",
      "Continers"
    ]
  },
  {
    "objectID": "2.3_MPI_stupidly_parrellel.html#parallel-jobs",
    "href": "2.3_MPI_stupidly_parrellel.html#parallel-jobs",
    "title": "Threads, MPI and embarrassingly parallel",
    "section": "Parallel Jobs",
    "text": "Parallel Jobs\nThere are a number of different scenarios in which you may want to parallelize your job:\n\nEmbarrassingly parallel\nMPI - a multi-process application (possibly across multiple compute hosts)\nmulti-threading - shared memory using OpenMP or perhaps pthreads.\nmultiple instances of a single application.\n…plus more scenarios but are probably out of scope of this tutorial.",
    "crumbs": [
      "Session 2: HPC Cloud Slurm",
      "Threads, MPI and embarrassingly parallel"
    ]
  },
  {
    "objectID": "2.3_MPI_stupidly_parrellel.html#embarrassingly-parallel",
    "href": "2.3_MPI_stupidly_parrellel.html#embarrassingly-parallel",
    "title": "Threads, MPI and embarrassingly parallel",
    "section": "Embarrassingly parallel",
    "text": "Embarrassingly parallel\nMany processes in genomics do the same task on large arrays of data. One of the simplest way of speeding up the process is to split up the input files and perfrom the same task multiple times at the same time - this is called an _‘Embarrassingly parallel’ task.\nLets do this for the blast example that we started in the last task. We can investigate whether there are any further methods of improving performance, and attempt to find a improvemet in the initial blast job’s solution to reduce the time taken. As it turns out, it is possible in this situation to split the input fasta file into a number of sections, and have an independent job acting on each of those sections. Each independent job could then be parallelized, say over 8 threads, and all jobs can run concurrently.\nWe create a job script (mammoth_8thread_split_1of4.sh) that will run blast command on the first file-section only.\nWe perform the following sequence of commands, first splitting the input fasta file into 4 parts, then creating the 4 independent job-scripts, and submit the jobs.\n\n## enter a interactive Job\nsrun -c 4 --mem=8G -p defq --pty bash\ncd ~/scratch/blast_test/\n\n## a new folder\nmkdir split-files4\ncd split-files4/\ncp ~/TAIR10_pep_4000.fasta  .\n#\n## split the fasta file into 4 equal(ish) sections\ncurl -L https://raw.githubusercontent.com/gmarnellos/Trinotate_example_supplement/master/split_fasta.pl | perl /dev/stdin  TAIR10_pep_4000.fasta  TAIR10_pep.vol 1000\n\ncreate script that will spawn blast_jobs (use nano or vi) - spawn_blastp.sh\n\n!#/bin/bash\n\nfor i in {1..4};do\n\nexport i\n\nsbatch blastp.sh\n\ndone\n\nmake script executable chmod +x spawn_blastp.sh\nEdit blast job so that each time it is called it uses a different section of the query file\n\n#!/bin/bash\n#SBATCH --partition=defq       # the requested queue\n#SBATCH --nodes=1              # number of nodes to use\n#SBATCH --tasks-per-node=1     # \n#SBATCH --cpus-per-task=8      #   \n#SBATCH --mem-per-cpu=11500    # in megabytes, unless unit explicitly stated\n#SBATCH --error=%J.err         # redirect stderr to this file\n#SBATCH --output=%J.out        # redirect stdout to this file\n##SBATCH --mail-user=[insert email address]@Cardiff.ac.uk  # email address used for event notification\n##SBATCH --mail-type=start                                 # email on job start  \n##SBATCH --mail-type=end                                   # email on job end\n##SBATCH --mail-type=fail                                  # email on job failure\n\necho \"Usable Environment Variables:\"\necho \"=============================\"\necho \"hostname=$(hostname)\"\necho \\$SLURM_JOB_ID=${SLURM_JOB_ID} \necho \\$SLURM_NTASKS=${SLURM_NTASKS}\necho \\$SLURM_NTASKS_PER_NODE=${SLURM_NTASKS_PER_NODE}\necho \\$SLURM_CPUS_PER_TASK=${SLURM_CPUS_PER_TASK}\necho \\$SLURM_JOB_CPUS_PER_NODE=${SLURM_JOB_CPUS_PER_NODE}\necho \\$SLURM_MEM_PER_CPU=${SLURM_MEM_PER_CPU}\n\ncat $0\n\nmodule load blast/2.12.0\n\nindir=\"/mnt/scratch/${USER}/blast_test/split-files4\"\n\ndbdir=\"/mnt/scratch/${USER}/blast_test\"\n\noutdir=\"/mnt/scratch/${USER}/blast_test/split-files4\"\n\ntime blastp -num_threads ${SLURM_CPUS_PER_TASK} \\\n            -query \"${indir}/TAIR10_pep.vol.${i}.fasta\" \\\n            -task blastp \\\n            -num_descriptions 16 \\\n            -num_alignments 1 \\\n            -db ${dbdir}/TAIR10_pep \\\n            -out \"${outdir}/blastp_vol${i}_cpu${SLURM_CPUS_PER_TASK}_job${SLURM_JOBID}.txt\"\n\nPerform file-splitting procedure for both a 2-split and a 4-split of the original fasta file. The ‘time-to-solution’ results are added to the original benchmark chart. We assume that all jobs run concurrently, and we take the wall-time for the longest job\n\n\n\n\n\n\nMPI Jobs\n\n\n\n\n\nOur example MPI job is based on a quantum espresso calculation. This script utilises the srun command, which is part of the slurm family of tools to run a parallel job on a cluster\n\n#!/bin/bash\n#SBATCH --partition=mammoth    # the requested queue\n#SBATCH --job-name=qe_mpi      # name the job           \n#SBATCH --nodes=1              # number of nodes to use\n#SBATCH --ntasks=32            # total number of tasks (processes)\n#SBATCH --mem-per-cpu=100      # in megabytes, unless unit explicitly stated\n#SBATCH --error=%J.err         # redirect stderr to this file\n#SBATCH --output=%J.out        # redirect stdout to this file\n##SBATCH --mail-user=[insert email address]@Cardiff.ac.uk  # email address used for event notification\n##SBATCH --mail-type=end                                   # email on job end\n##SBATCH --mail-type=fail                                  # email on job failure\n\nmodule load  qe/6.0\n\necho \"Usable Environment Variables:\"\necho \"=============================\"\necho \"hostname=$(hostname)\"\necho \\$SLURM_JOB_ID=${SLURM_JOB_ID}\necho \\$SLURM_NTASKS=${SLURM_NTASKS}\necho \\$SLURM_NTASKS_PER_NODE=${SLURM_NTASKS_PER_NODE}\necho \\$SLURM_CPUS_PER_TASK=${SLURM_CPUS_PER_TASK}\necho \\$SLURM_JOB_CPUS_PER_NODE=${SLURM_JOB_CPUS_PER_NODE}\necho \\$SLURM_MEM_PER_CPU=${SLURM_MEM_PER_CPU}\necho \"module list:\"\nmodule list 2&gt;&1\n\n# Some of these environment variables are utilised by the qe executable itself\nexport ESPRESSO_DATAPATH=~/classdata/REFS/slurm/slurm_examples/example2_mpi/\nexport ESPRESSO_PSEUDO=${ESPRESSO_DATAPATH}\nexport ESPRESSO_TMPDIR=${ESPRESSO_DATAPATH}/${SLURM_JOB_ID}\n\n# handy to place this in job output for future reference...\ncat ${ESPRESSO_DATAPATH}/atom.in\n\n# execute the parallel job (we also time it)\ntime srun -n ${SLURM_NTASKS} pw.x &lt; ${ESPRESSO_DATAPATH}/atom.in &gt; atom.job${SLURM_JOB_ID}.out\n\nThe job requests 32 cores to be allocated, and runs the srun command with the argument -n ${SLURM_NTASKS} which tells srun to spawn the mpi-job with the total number of processes requested. Quantum Espresso utilises the environment variable ESPRESSO_TMPDIR which points to a temporary folder. We design this in our slurm script to point to a subfolder.\nAn alternative storage location is the compute node’s local storage. This can improve runtime I/O performance. However, local storage on the compute nodes is limited (Gigabytes not Terabytes), and it’s availability is a little hidden from the user, so take care not to fill up the disk(!) and remove all files from the compute node’s local storage within the job script (your only access to the compute node’s /local folder is via the slurm script). An alternative job script which utilises a compute node’s /local storage is provided on the gomphus server /mnt/clusters/sponsa/data/classdata/Bioinformatics/REFS/slurm/slurm_examples/example2_mpi/example2_mpi_localstorage.sh.\n\n\n\n\n\n\n\n\n\nThreaded Jobs\n\n\n\n\n\nA number of popular bioinformatics software are capable of parallelising execution using threads (usually OpenMP or pthreads). This parallelisation method does not normally use distributed memory, so the application will need to be run on a single node. Our threaded example slurm-script is based on BLAST+. The job script is listed:\n\n#!/bin/bash\n#SBATCH --partition=defq\n#SBATCH --nodes=1\n#SBATCH --tasks-per-node=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem-per-cpu=2000\n#SBATCH --error=%J.err\n#SBATCH --output=%J.out\n##SBATCH --mail-type=end\n##SBATCH --mail-user=[your.email@address]\n\n\n# Example slurm script\n#  This script is a little wasteful of resources,\n#  but demonstrates a simple pipeline.\n#  \n#  For a more efficient use of resources, please consider\n#  running the pipeline as a series of jobs (chain-jobs).\n\n\necho \"Usable Environment Variables:\"\necho \"=============================\"\necho \"hostname=$(hostname)\"\necho \\$SLURM_JOB_ID=${SLURM_JOB_ID}\necho \\$SLURM_NTASKS=${SLURM_NTASKS}\necho \\$SLURM_NTASKS_PER_NODE=${SLURM_NTASKS_PER_NODE}\necho \\$SLURM_CPUS_PER_TASK=${SLURM_CPUS_PER_TASK}\necho \\$SLURM_JOB_CPUS_PER_NODE=${SLURM_JOB_CPUS_PER_NODE}\necho \\$SLURM_MEM_PER_CPU=${SLURM_MEM_PER_CPU}\necho \"module list:\"\nmodule list 2&gt;&1\n\nDATAFOLDER=~/classdata/REFS/slurm/slurm_examples/example3_pipeline\n\n\n### The data used in this pipeline has already been downloaded and stored in $DATAFOLDER.\n### Here are the commands used to download the data...\n# cd $DATAFOLDER\n# curl -LO https://data.broadinstitute.org/Trinity/Trinotate_v2.0_RESOURCES/uniprot_sprot.trinotate_v2.0.pep.gz\n# gunzip uniprot_sprot.trinotate_v2.0.pep.gz\n# curl -LO https://data.broadinstitute.org/Trinity/Trinotate_v2.0_RESOURCES/Pfam-A.hmm.gz\n# gunzip Pfam-A.hmm.gz\n# curl -L -o Trinotate.sqlite.gz https://data.broadinstitute.org/Trinity/Trinotate_v2.0_RESOURCES/Trinotate.sprot_uniref90.20150131.boilerplate.sqlite.gz\n# gunzip Trinotate.sqlite.gz\n# curl -LO ftp://ftp.ensembl.org/pub/release-82/fasta/mus_musculus/cdna/Mus_musculus.GRCm38.cdna.all.fa.gz\n# gunzip Mus_musculus.GRCm38.cdna.all.fa.gz\n# curl -LOgz https://github.com/gmarnellos/Trinotate_example_supplement/raw/master/mouse38_cdna.fa.gz\n# gunzip mouse38_cdna.fa.gz\n\n# Now we get on to the pipeline\n\n# make a link to all datafiles\nfor f in ${DATAFOLDER}/* ; do ln -s $f ; done\n\n#Index the SwissProt database for use with blast\n\nmodule load blast\nmakeblastdb -version\nmakeblastdb -in uniprot_sprot.trinotate_v2.0.pep -dbtype prot\nmodule unload blast\n\n# Prepare the Pfam database for use with hmmscan\nmodule load hmmer\nhmmpress -h\nhmmpress Pfam-A.hmm\nmodule load hmmer\n\n# Use Transdecoder to produce the most likely longest-ORF peptide candidates\nmodule load TransDecoder/v3.0.1\nTransDecoder.LongOrfs -t mouse38_cdna.fa\nTransDecoder.Predict -t mouse38_cdna.fa\nmodule unload TransDecoder/v3.0.1\n\nmodule load blast\nblastx -query mouse38_cdna.fa -db uniprot_sprot.trinotate.pep -num_threads ${SLURM_CPUS_PER_TASK} -max_target_seqs 1 -outfmt 6 &gt; blastx.vol.outfmt6\nblastp -query mouse38_cdna.fa.transdecoder.pep -db uniprot_sprot.trinotate_v2.0.pep -num_threads ${SLURM_CPUS_PER_TASK} -max_target_seqs 1 -outfmt 6 &gt; blastp.vol.outfmt6\nmodule unload blast\n\n# Identify protein domains\nmodule load hmmer/3.1b2\nhmmscan --cpu ${SLURM_CPUS_PER_TASK} --domtblout TrinotatePFAM.out Pfam-A.hmm mouse38_cdna.fa.transdecoder.pep &gt; pfam.log\nmodule unload hmmer/3.1b2\n\n# Produce the Gene/Transcript relationship\ngrep \"^&gt;\" Mus_musculus.GRCm38.cdna.all.fa   | perl -p -e 's/^&gt;(\\S+).*\\s+gene:(ENSMUSG\\d+).*$/$2\\t$1/' &gt; gene_transcript_map.txt\n\n# Now populate the sqlite database\nmodule load Trinotate/v3.0.1\nTrinotate Trinotate.sqlite init --gene_trans_map gene_transcript_map.txt --transcript_fasta mouse38_cdna.fa --transdecoder_pep mouse38_cdna.fa.transdecoder.pep\nTrinotate Trinotate.sqlite LOAD_swissprot_blastp blastp.vol.outfmt6\nTrinotate Trinotate.sqlite LOAD_swissprot_blastx blastx.vol.outfmt6\nTrinotate Trinotate.sqlite LOAD_pfam TrinotatePFAM.out\n# Create the annotation report\nTrinotate Trinotate.sqlite report -E 0.00001 &gt; trinotate_annotation_report.xls\nmodule unload Trinotate/v3.0.1\n\nThis is quite a busy job-script (and also inefficient on resources!). It runs through a number of steps, but some of those steps will utilise parallelisation via threading, and use the slurm environment variable SLURM_CPUS_PER_TASK to inform the application(s) of the correct number of threads.\nBut why is this job inefficient on resources? This particular job involves a number of steps: some utilising parallelisation, and some not; some memory-hungry, others not. The problem with this is that the job has allocated to it a set amount of resources (compute and memory), which is allocated to it for the lifetime of the job. But only at certain times in this job are the resources requested fully utilised. At all other times this job is running, the resources are allocated, but not used, and therefore making those resources unavailable to other jobs. This has a knock-on effect of increasing queue-times, and leaves expensive resources idle.\nA much more efficient way of running the same pipeline is to chain the job - split the pipeline into component parts and submit separate jobs for each of those parts. Each section of the pipeline (having its own job-script) is then free to allocate resources specific to that section of the pipeline. In the slurm world this is called job chaining, and has been exemplified in the next section using the same pipeline.\n\nJob Chains and Job Dependency\nChaining jobs is a method of sequentially running dependent jobs. Our chain-job example is a pipeline of 6 separate job scripts, based on the blast+ pipeline of the previous section. We do not show the full six job-scripts here for brevity, but are available on the gomphus cluster under /mnt/clusters/sponsa/data/classdata/Bioinformatics/REFS/slurm/slurm_examples/example4_chain.\nSlurm has an option -d or –dependency that allows to specify that a job is only permitted to start if another job finished successfully.\nIn the folder (gomphus cluster) ~/classdata/Bioinformatics/REFS/slurm/slurm_examples/example4_chain there are 6 separate job-scripts that need to be executed in a certain order. They are numbered in the correct pipeline order:\n\n[user@gomphus ~]$ tree  ~/classdata/Bioinformatics/REFS/slurm/slurm_examples/example4_chain\n~/classdata/Bioinformatics/REFS/slurm/slurm_examples/example4_chain\n├── example4_chain-step1.sh\n├── example4_chain-step2.sh\n├── example4_chain-step3.sh\n├── example4_chain-step4.sh\n├── example4_chain-step5.sh\n├── example4_chain-step6.sh\n├── example4_submit_all.sh\n├── mouse38_cdna.fa\n├── Mus_musculus.GRCm38.cdna.all.fa\n├── Pfam-A.hmm\n├── pipeline1.sh\n├── Trinotate.sqlite\n└── uniprot_sprot.trinotate_v2.0.pep\n\n0 directories, 13 files\n\nEach job is (importantly) commonly named using #SBATCH –job-name within each job-script. Also within this folder is a simple script (example4_submit_all.sh) that will execute the sbatch command on each of the job-scripts in the correct order:\n\n#!/bin/bash:\n\nfor c in ~/classdata/REFS/slurm/slurm_examples/example4_chain/example4_chain-step?.sh ;\ndo\n sbatch -d singleton $c\ndone\n\nThis sbatch command uses the -d singleton flag to notify slurm of the job-dependencies (all jobs must have the name job name defined by #SBATCH --job-name [some constant name]. At which point each submitted job will be forced to depend on successful completion of any previous job submitted by the same user, and with the same job-name. The full pipeline of 6 jobs will now run to completion, with no further user-intervention, making efficient use of the available resources.",
    "crumbs": [
      "Session 2: HPC Cloud Slurm",
      "Threads, MPI and embarrassingly parallel"
    ]
  },
  {
    "objectID": "2.3_MPI_stupidly_parrellel.html#job-chains-and-job-dependency",
    "href": "2.3_MPI_stupidly_parrellel.html#job-chains-and-job-dependency",
    "title": "Threads, MPI and embarrassingly parallel",
    "section": "Job Chains and Job Dependency",
    "text": "Job Chains and Job Dependency\nChaining jobs is a method of sequentially running dependent jobs. Our chain-job example is a pipeline of 6 separate job scripts, based on the blast+ pipeline of the previous section. We do not show the full six job-scripts here for brevity, but are available on the gomphus cluster under /mnt/clusters/sponsa/data/classdata/Bioinformatics/REFS/slurm/slurm_examples/example4_chain.\nSlurm has an option -d or –dependency that allows to specify that a job is only permitted to start if another job finished successfully.\nIn the folder (gomphus cluster) ~/classdata/Bioinformatics/REFS/slurm/slurm_examples/example4_chain there are 6 separate job-scripts that need to be executed in a certain order. They are numbered in the correct pipeline order:\n\n[user@gomphus ~]$ tree  ~/classdata/Bioinformatics/REFS/slurm/slurm_examples/example4_chain\n~/classdata/Bioinformatics/REFS/slurm/slurm_examples/example4_chain\n├── example4_chain-step1.sh\n├── example4_chain-step2.sh\n├── example4_chain-step3.sh\n├── example4_chain-step4.sh\n├── example4_chain-step5.sh\n├── example4_chain-step6.sh\n├── example4_submit_all.sh\n├── mouse38_cdna.fa\n├── Mus_musculus.GRCm38.cdna.all.fa\n├── Pfam-A.hmm\n├── pipeline1.sh\n├── Trinotate.sqlite\n└── uniprot_sprot.trinotate_v2.0.pep\n\n0 directories, 13 files\n\nEach job is (importantly) commonly named using #SBATCH –job-name within each job-script. Also within this folder is a simple script (example4_submit_all.sh) that will execute the sbatch command on each of the job-scripts in the correct order:\n\n#!/bin/bash:\n\nfor c in ~/classdata/REFS/slurm/slurm_examples/example4_chain/example4_chain-step?.sh ;\ndo\n sbatch -d singleton $c\ndone\n\nThis sbatch command uses the -d singleton flag to notify slurm of the job-dependencies (all jobs must have the name job name defined by #SBATCH --job-name [some constant name]. At which point each submitted job will be forced to depend on successful completion of any previous job submitted by the same user, and with the same job-name. The full pipeline of 6 jobs will now run to completion, with no further user-intervention, making efficient use of the available resources.",
    "crumbs": [
      "Session 2: HPC Cloud Slurm",
      "Threads, MPI and embarrassingly parallel"
    ]
  },
  {
    "objectID": "2.1_HPC_Cloud_Slurm.html#house-keeping",
    "href": "2.1_HPC_Cloud_Slurm.html#house-keeping",
    "title": "HPC, Cloud Computing & Job Schedulers",
    "section": "House Keeping",
    "text": "House Keeping\n\nlogin to the server\nHPC system available to big data masters students:\nhost\n\ngomphus.bios.cf.ac.uk\n\nUser name university username Password SSO Password Port 22\nWe suggest you use (MobaXterm)[https://mobaxterm.mobatek.net/] (PC) and terminal SSH (Mac) to access the server and MobaXtrem and (cyberduck)[https://cyberduck.io/] or (filezilla)[https://filezilla-project.org/] to allow secure file transfer (sftp) between server and local computer resources.\n\n\n\ncreate ‘symlinks’ to classdata and my data\nLinux ln command allows you to create a symbolic link (also symlink or soft link) to a file or directory (called the “target”) by specifying a path thereto. The format is ln -s [target location] [shortcut]. I recommend you create the following symlinks at your root position.\n\nln -s /mnt/clusters/sponsa/data/classdata classdata\n\nln -s /mnt/clusters/sponsa/data/${USER} mydata\n\nmkdir /mnt/scratch/${USER}\n\nln -s /mnt/scratch/${USER} scratch\n\n\n\nYou should now see two sym links at you root classdata and mydata that will allow you to address these locations using:\n~/classdata/\n~/mydata/",
    "crumbs": [
      "Session 2: HPC Cloud Slurm",
      "HPC, Cloud Computing & Job Schedulers"
    ]
  },
  {
    "objectID": "2.1_HPC_Cloud_Slurm.html#explore-your-hpc-resources",
    "href": "2.1_HPC_Cloud_Slurm.html#explore-your-hpc-resources",
    "title": "HPC, Cloud Computing & Job Schedulers",
    "section": "Explore your HPC resources",
    "text": "Explore your HPC resources\n\nYour disc quota\n\nReview what ‘storage’ resources are available to you.\nquota -us [username]\n\nThis should display the following information:\nDisk quotas for user smbpk (uid 31961):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFilesystem\nspace\nquota\nlimit\ngrac\nfiles\nquota\nlimit\ngrace\n\n\n\n\n/dev/mapper/cl-root\n32K\n5120M\n7168M\n\n13\n0\n0\n\n\n\n192.168.2.41:/mnt/data/clusters/anax\n4K\n1978G\n2078G\n\n6\n0\n0\n\n\n\n\n\nCheck your personal disc usage in mydata (du is the disc utility -h = human readable, -s = summarise )\ndu -sh ~/mydata/\n\n\n\nHPC resources\nThe head node on gomphus is a 16 cpu / 32 Gb RAM server…..but it provides access to a series of servers.\n\nNode Availability\nThe head node has a specific script that displays the resources available to you. Run the command:\ngomphus.node_availability.sh\nThis should display the following information:\nKey ('CPUS' column):\n  A = # cores Allocated\n  I = # cores Idle\n  O = # cores not 'Idle' or 'Allocated'\n  T = Total # of cores\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNODELIST\nNODES\nPARTITION\nSTATE\nCPUS\nS:C:T\nMEMORY\nTMP_DISK\nWEIGHT\nAVAIL_FE\nCPUS(A/I/O/T)\nREASON\n\n\n\n\ngomphus2\n1\ndefq*\nidle\n64\n1:64:1\n128000\n0\n1\n(null)\n0/64/0/64\nnone\n\n\ngomphus3\n1\ndefq*\nidle\n64\n1:64:1\n440000\n0\n1\n(null)\n0/64/0/64\nnone\n\n\ngomphus1\n1\ndefq*\nidle\n64\n1:32:1\n128000\n0\n1\n(null)\n0/64/0/64\nnone",
    "crumbs": [
      "Session 2: HPC Cloud Slurm",
      "HPC, Cloud Computing & Job Schedulers"
    ]
  },
  {
    "objectID": "2.1_HPC_Cloud_Slurm.html#the-slurm-job-scheduler",
    "href": "2.1_HPC_Cloud_Slurm.html#the-slurm-job-scheduler",
    "title": "HPC, Cloud Computing & Job Schedulers",
    "section": "The Slurm Job Scheduler",
    "text": "The Slurm Job Scheduler\nThe slurm job scheduler is software that controls and monitors access to high-performance computing hardware. It allocates resources (cpus/cores/RAM) on a per-job basis.",
    "crumbs": [
      "Session 2: HPC Cloud Slurm",
      "HPC, Cloud Computing & Job Schedulers"
    ]
  },
  {
    "objectID": "2.1_HPC_Cloud_Slurm.html#slurm-partitions-queues",
    "href": "2.1_HPC_Cloud_Slurm.html#slurm-partitions-queues",
    "title": "HPC, Cloud Computing & Job Schedulers",
    "section": "Slurm Partitions (Queues)",
    "text": "Slurm Partitions (Queues)\nA number of slurm partitions (queues) may exist on any slurm cluster, and each queue will suit certain types of jobs. Each of the separate queues will handle jobs slightly differently, or will have varying resources available to it. Below is a list of queues available on our HPC systems, and the typical types of jobs they accommodate.\nGomphus Queues\n\n\n\n\n\n\n\n\n\npartition(queue) name\ndefault?\ndescription\n\n\n\n\n\ndefq\n✔\nbatch jobs, low mem/cpu requirement",
    "crumbs": [
      "Session 2: HPC Cloud Slurm",
      "HPC, Cloud Computing & Job Schedulers"
    ]
  },
  {
    "objectID": "2.1_HPC_Cloud_Slurm.html#submitting-jobs-the-slurm-job-script",
    "href": "2.1_HPC_Cloud_Slurm.html#submitting-jobs-the-slurm-job-script",
    "title": "HPC, Cloud Computing & Job Schedulers",
    "section": "Submitting Jobs (The Slurm Job-Script)",
    "text": "Submitting Jobs (The Slurm Job-Script)\nThe typical method to start a job on a slurm cluster is to first create a job-script, then submit that job-script to the slurm queue using the sbatch command. The job-script is laid out in a certain way (examples below), which will first request resources on the compute nodes, and then define the individual steps of your job. Below is an example job script for gomphus slurm using the partition defq. We’ll name the job-script submit.sh:\n\n#!/bin/bash\n#SBATCH --partition=defq       # the requested queue\n#SBATCH --nodes=1              # number of nodes to use\n#SBATCH --tasks-per-node=1     #\n#SBATCH --cpus-per-task=1      #   \n#SBATCH --mem-per-cpu=1000     # in megabytes, unless unit explicitly stated\n#SBATCH --error=%J.err         # redirect stderr to this file\n#SBATCH --output=%J.out        # redirect stdout to this file\n##SBATCH --mail-user=[insert email address]@Cardiff.ac.uk  # email address used for event notification\n##SBATCH --mail-type=end                                   # email on job end\n##SBATCH --mail-type=fail                                  # email on job failure\n\necho \"Some Usable Environment Variables:\"\necho \"=================================\"\necho \"hostname=$(hostname)\"\necho \\$SLURM_JOB_ID=${SLURM_JOB_ID}\necho \\$SLURM_NTASKS=${SLURM_NTASKS}\necho \\$SLURM_NTASKS_PER_NODE=${SLURM_NTASKS_PER_NODE}\necho \\$SLURM_CPUS_PER_TASK=${SLURM_CPUS_PER_TASK}\necho \\$SLURM_JOB_CPUS_PER_NODE=${SLURM_JOB_CPUS_PER_NODE}\necho \\$SLURM_MEM_PER_CPU=${SLURM_MEM_PER_CPU}\n\n# Write jobscript to output file (good for reproducibility)\ncat $0\n\nThe job can be submitted from the head node of the slurm cluster using the command:\nsbatch submit.sh\nThis particular job does nothing interesting except print out some useful job-specific slurm environment variables, which you may utilise in your job-scripts.\nThis job-script is actually just a simple bash script, and in bash the # character will comment out the remaining text on that line, and will not be parsed by the bash interpreter. In a slurm job-script however, the lines starting with #SBATCH is parsed by slurm before the script is handed over to the bash interpreter. The #SBATCH lines will inform the slurm scheduler of the amount of resources requested for the job.\nIf you would like to comment out certain slurm commands in your job-script, simply add an extra # at the beginning of the #SBATCH lines (as we have done in the above example for all email-related commands).\nSome further useful (but not exhaustive) SBATCH options are detailed:\n\n\n\n\n\n\n\nSBATCH option\nDescription\n\n\n\n\n–ntasks-per-node\nallocate n tasks per allocated node.\n\n\n–nodes\nset the number of nodes that the job will require. Each node will have –ntasks-per-node processes started\n\n\n–ntasks\nspawn n tasks (default=1). This determines how many processes will be spawned (for MPI jobs).\n\n\n–cpus-per-task\nallocate n CPUs per task. This option is needed for multi-threaded jobs. Typically n should be equal to the number of threads your program spawns.\n\n\n–partition\nselect the partition (queue) where you want to execute your job. Depending on the HPC system you are using, there may, or may not, be much choice.\n\n\n–mem-per-cpu\nspecify the the memory needed per allocated CPU in MB.\n\n\n–job-name\nname your job. This name will be shown in the queue status and email alerts. It is also important for job chaining.\n\n\n–output\nspecify a filename that will be used to store the stdout of your job. The slurm variable %J is useful here which slurm will interpret as the job_id of the job.\n\n\n–error\nsimilar to the –output option, but redirect your job’s stderr.\n\n\n\n\nResource Limitations\nOn the a slurm cluster, the slurm scheduler keeps track of both requested CPUs, and memory. If your job exceeds the currently available resources then your job will be queued until sufficient resources are available. If your job exceeds the total amount of resources available on the compute nodes then your job submission will fail and will not be queued, in which case an error will be returned.",
    "crumbs": [
      "Session 2: HPC Cloud Slurm",
      "HPC, Cloud Computing & Job Schedulers"
    ]
  },
  {
    "objectID": "2.1_HPC_Cloud_Slurm.html#my-first-slurm-job---excersise",
    "href": "2.1_HPC_Cloud_Slurm.html#my-first-slurm-job---excersise",
    "title": "HPC, Cloud Computing & Job Schedulers",
    "section": "My first slurm Job - Excersise",
    "text": "My first slurm Job - Excersise\nTo submit an appropriately configured job-script to the slurm queue, use the command:\n\nsbatch submit.sh\n\nUse cat or less to rReview the content of the .out and .err files - note the .out file should contain a summary of the varibles used by the script and the program you ran.\nThe gomphus cluster has a number of example job-scripts (located under ~/classdata/REFS/slurm on the gompus.bios.cf.ac.uk server). You may submit these jobs with the following commands on the gomphus cluster:\n\n# first cd to your allocated ~/mydata/ folder.\nmkdir -p test_jobs && cd test_jobs\ncp ~/classdata/REFS/slurm/slurm_examples/example1/* .\nsbatch example1.sh\n\nWhat is the function of this example script ??",
    "crumbs": [
      "Session 2: HPC Cloud Slurm",
      "HPC, Cloud Computing & Job Schedulers"
    ]
  },
  {
    "objectID": "2.1_HPC_Cloud_Slurm.html#loading-programs-in-slurm-scripts",
    "href": "2.1_HPC_Cloud_Slurm.html#loading-programs-in-slurm-scripts",
    "title": "HPC, Cloud Computing & Job Schedulers",
    "section": "Loading programs in slurm scripts",
    "text": "Loading programs in slurm scripts\nReview what programs are available too you using\n\nmodule avail\n\nPrograms should be loaded using ‘module load’ after the slurm parameters have been defined and before you define variable or provide commands for you program.\nmodule list for gomphus (2022)\n\n------------------------------------------------------- /mnt/clusters/sponsa/software/nospack/modulefiles --------------------------------------------------------\napptainer/1.3.3       checkm2/1.0.1-conda  fmlrc/v1.0.0  java/17.0.1          metabat2/2.17-conda  reportseff/v2.7.6  snippy/v4.6.0\nartemis/18.2.0-conda  coverm/0.7.0-conda   go/1.22.4     MAGScoT/1.0.0-conda  mitos/2.0.4-conda    rnammer/1.2        snp-sites/2.5.1\nbioconvert/1.1.1      fastqc/v0.12.1       java/11.0.2   maxbin2/2.2.7-conda  qiime2/2024.5-conda  snakemake/8.20.6   star/2.7.11b-conda\n\n-------------------------------------------- /mnt/clusters/sponsa/software/spack/share/spack/modules/linux-rocky8-zen --------------------------------------------\nabricate/1.0.0-hyc7x7h     caliper/2.11.0-ebmrucn        gatk/4.5.0.0-ah5vlbz          picard/3.1.1-6z7e4l7        python/3.11.7-vwaruyl     vt/0.5772-hm4aagu\nabyss/2.3.5-n7ic7f5        cdhit/4.8.1-kljs53a           grace/5.1.25-6ff2ofq          pilon/1.22-4m44rwk          qualimap/2.2.1-5n2xovw    wtdbg2/2.3-zh2scui\nalan/2.1.1-m3s7v46         dos2unix/7.4.4-lbswpzc        gromacs/2022.6-55ulebd        prokka/1.14.6-mframqo       r/4.4.1-tbzwogp\nbarrnap/0.9-ypa5cto        dyninst/13.0.0-7ddr5br        igv/2.16.2-izjk4b4            py-macs3/3.0.0b3-hzy2kys    raxml-ng/1.1.0-af5tz6w\nbcftools/1.19-ugoqbig      emboss/6.6.0-dyziyqa          kraken2/2.1.2-dj6czbo         py-multiqc/1.15-hcx2o2j     seqtk/1.4-emavr63\nbedtools2/2.31.1-z2zkxtk   fastp/0.23.4-rsrnyej          mafft/7.505-zhsnti2           py-panaroo/1.2.10-ngp4lzx   spades/3.15.5-yabtygn\nblast-plus/2.14.1-qxsspbk  fasttree/2.1.11-ca5e4pn       mcl/14-137-seoyf2k            py-quast/5.2.0-az54c2i      subread/2.0.6-hbzwdut\nbowtie/1.3.1-hcmaf4e       fastx-toolkit/0.0.14-td2y4ek  minimap2/2.28-mpg3455         py-unicycler/0.5.0-b3qlto7  tmux/3.4-goii5an\nbowtie2/2.5.2-xec7g7r      figtree/1.4.4-lbef6u3         perl-xml-simple/2.25-3txxqfi  python/3.7.4-j2ydjpr        trimmomatic/0.39-gkur3wo\nbusco/5.4.3-y2iac7v        freebayes/1.3.6-fwyhw7e       perl/5.34.0-3gr3ksd           python/3.10.5-g77cmwy       velvet/1.2.10-z6f62cw\n\n------------------------------------------------------- /mnt/clusters/gomphus/software/nospack/modulefiles -------------------------------------------------------\napptainer/1.3.3       charliecloud/0.38    fmlrc/v1.0.0  MAGScoT/1.0.0-conda  nextflow/23.10.0     rnammer/1.2       star/2.7.11b-conda\napptainer/1.3.4       checkm2/1.0.1-conda  go/1.22.4     maxbin2/2.2.7-conda  nextflow/24.04.4     snakemake/8.20.6\nartemis/18.2.0-conda  coverm/0.7.0-conda   java/11.0.2   metabat2/2.17-conda  qiime2/2024.5-conda  snippy/v4.6.0\nbioconvert/1.1.1      fastqc/v0.12.1       java/17.0.1   mitos/2.0.4-conda    reportseff/v2.7.6    snp-sites/2.5.1\n\n------------------------------------------- /mnt/clusters/gomphus/software/spack/share/spack/modules/linux-rocky8-zen --------------------------------------------\nabricate/1.0.0-arbowjh     caliper/2.11.0-ebmrucn        gatk/4.5.0.0-ah5vlbz          picard/3.1.1-6z7e4l7        python/3.11.7-vwaruyl     vt/0.5772-hm4aagu\nabyss/2.3.5-n7ic7f5        cdhit/4.8.1-kljs53a           grace/5.1.25-4rbzfg5          pilon/1.22-4m44rwk          qualimap/2.2.1-5n2xovw    wtdbg2/2.3-zh2scui\nalan/2.1.1-m3s7v46         dos2unix/7.4.4-lbswpzc        gromacs/2022.6-55ulebd        prokka/1.14.6-thd4tzv       r/4.4.1-p34wi5a\nbarrnap/0.9-ypa5cto        dyninst/13.0.0-7ddr5br        igv/2.16.2-izjk4b4            py-macs3/3.0.0b3-hzy2kys    raxml-ng/1.1.0-af5tz6w\nbcftools/1.19-kxefepm      emboss/6.6.0-jyqjajy          kraken2/2.1.2-dj6czbo         py-multiqc/1.15-rrs5gki     seqtk/1.4-emavr63\nbedtools2/2.31.1-z2zkxtk   fastp/0.23.4-rsrnyej          mafft/7.505-zhsnti2           py-panaroo/1.2.10-aio7ggr   spades/3.15.5-yabtygn\nblast-plus/2.14.1-ad3m247  fasttree/2.1.11-ca5e4pn       mcl/14-137-seoyf2k            py-quast/5.2.0-ahwdgc2      subread/2.0.6-hbzwdut\nbowtie/1.3.1-hcmaf4e       fastx-toolkit/0.0.14-td2y4ek  minimap2/2.28-mpg3455         py-unicycler/0.5.0-gayhlsi  tmux/3.4-goii5an\nbowtie2/2.5.2-xec7g7r      figtree/1.4.4-lbef6u3         perl-xml-simple/2.25-3txxqfi  python/3.7.4-j2ydjpr        trimmomatic/0.39-gkur3wo\nbusco/5.4.3-lhb6rj4        freebayes/1.3.6-fwyhw7e       perl/5.34.0-3gr3ksd           python/3.10.5-g77cmwy       velvet/1.2.10-z6f62cw",
    "crumbs": [
      "Session 2: HPC Cloud Slurm",
      "HPC, Cloud Computing & Job Schedulers"
    ]
  },
  {
    "objectID": "2.1_HPC_Cloud_Slurm.html#viewing-information-about-a-job",
    "href": "2.1_HPC_Cloud_Slurm.html#viewing-information-about-a-job",
    "title": "HPC, Cloud Computing & Job Schedulers",
    "section": "Viewing Information About a Job",
    "text": "Viewing Information About a Job\n\nInformation on Running Jobs\nIf your job-script used the srun command to kick off a (parallel) process, then slurm will be able to provide live information about your running job. All information about your running job can be listed with the sstat command:\n\nsstat -l -j [jobid]\n\nIn particular it may be useful to compare the fields MaxRSS and ReqMem. These fields report the actual max memory of a single task, and the requested memory of a single task, respectively. You may then tune any future job-scripts to more accurately represent your jobs, which will lead to better queue efficiency - meaning your job will likely start sooner.\nIf any application call within your job-script did not use the srun command, then no live information will be available. Instead, wait for your job to finish and use the sacct command, as described below.\n\n\nInformation on Completed Jobs\nSimilar to the sstat command used for running jobs, the sacct command will provide equivalent information about completed jobs.",
    "crumbs": [
      "Session 2: HPC Cloud Slurm",
      "HPC, Cloud Computing & Job Schedulers"
    ]
  },
  {
    "objectID": "2.1_HPC_Cloud_Slurm.html#useful-slurm-related-commands",
    "href": "2.1_HPC_Cloud_Slurm.html#useful-slurm-related-commands",
    "title": "HPC, Cloud Computing & Job Schedulers",
    "section": "Useful Slurm-Related Commands",
    "text": "Useful Slurm-Related Commands\nThe sinfo command can supply a lot of useful information about the nodes and available resources. A useful set of parameters of which to call sinfo with is the following:\n\nsinfo -o \"%24N %.5D %9P %11T %.4c %.8z %.8m %.8d %.6w %8f %15C %20E\"\nNODELIST                 NODES PARTITION STATE       CPUS    S:C:T   MEMORY TMP_DISK WEIGHT AVAIL_FE CPUS(A/I/O/T)   REASON              \ngomphus[1-4]                 4 defq*     idle          32   1:32:1    62250        0      1 (null)   0/128/0/128     none \n\nHere, we see a list of nodes on the gomphus cluster (grouped by partition, and by state), and information about the nodes. In this particular instance we see that gomphus has 4 nodes [1-4] each with 32 CPUS. The CPUS(A/I/O/T) column shows that the defq partition has a total of 0 CPUs Allocated, 128 CPUs Idle, 0 in state Other, making a Total of 128 (all 4 nodes combined).\nThe above command is a bit of a handful. You have already used the wrapper script created for you gomphus.job_history.sh\n\nother useful commands\nTo cancel a running job:\n\n$ scancel [jobid]\n\ninformation about the compute nodes:\n\n$ sinfo -lNe\n\nList the current status of the slurm queue:\n\n$ squeue\n\nFind information on previously completed jobs:\n\n$ sacct\n\nList information on running jobs:\n\n$ sstat",
    "crumbs": [
      "Session 2: HPC Cloud Slurm",
      "HPC, Cloud Computing & Job Schedulers"
    ]
  },
  {
    "objectID": "2.1_HPC_Cloud_Slurm.html#constructing-a-slurm-job---good-practice",
    "href": "2.1_HPC_Cloud_Slurm.html#constructing-a-slurm-job---good-practice",
    "title": "HPC, Cloud Computing & Job Schedulers",
    "section": "Constructing a slurm Job - good practice",
    "text": "Constructing a slurm Job - good practice\nAdapt RNAseq processing script 1-QC.sh (see session Session5 -RNAseq-Processing) to run under slurm (do not run script 2 – building genome) ** if you find this easy try adapting script 3,4 and 5.\nYou may want to discuss the resources required for the job.\nHints:\ndefine directories using full path derived with PWD -P\n\n~/mydata/ = /mnt/clusters/sponsa/data/$USER/\n\nWhen creating composite variables use enclose them in quotation marks to ensure they expand.\n\n\"${indir}/fastq/${i}_1.fastq\"",
    "crumbs": [
      "Session 2: HPC Cloud Slurm",
      "HPC, Cloud Computing & Job Schedulers"
    ]
  },
  {
    "objectID": "2.1_HPC_Cloud_Slurm.html#interactive-jobs",
    "href": "2.1_HPC_Cloud_Slurm.html#interactive-jobs",
    "title": "HPC, Cloud Computing & Job Schedulers",
    "section": "Interactive Jobs",
    "text": "Interactive Jobs\nEnter an interactive job and use this to review the files created by the RNAseq processing scripts.\n\n##Use any available node\nsrun -c 4 --mem=8G -p defq --pty bash\n\n##use a specific node - in this example called gomphus3\n\nsrun -c 4 --mem=8G -p defq --nodelist=gomphus3 --pty bash\n\nRemember to exit the interactive job when you are finished by typing exit\nAdvanced: Some HPC architectures will allow you to directly write to the ‘local’ harddrive /tmp For jobs with lots of I/O and where the nodes are using new M2 SSD drives this can accelerate certain types of jobs.",
    "crumbs": [
      "Session 2: HPC Cloud Slurm",
      "HPC, Cloud Computing & Job Schedulers"
    ]
  },
  {
    "objectID": "2.1_HPC_Cloud_Slurm.html#blast-as-an-example",
    "href": "2.1_HPC_Cloud_Slurm.html#blast-as-an-example",
    "title": "HPC, Cloud Computing & Job Schedulers",
    "section": "blast as an example",
    "text": "blast as an example\nWe take a closer look at running jobs on our cluster, we should provide examples of benchmarking code in order to find how to best utilise the resources available.\nWe start off by benchmarking a simple blast job, initially running over a single core, then parallelizing the job using blast’s built-in threading. All jobs are run on a single node, and we test a number of nodes and compare results. For example source code, see the folder ~/classdata/REFS/slurm/slurm_examples/indepth_parallelizing_blastp on the gomphus server.\nRemember to run all commands in your ~/mydata/[directory].\nFirst we obtain the sample data:\n\n# this is already downloaded to ~/classdata/REFS/slurm/slurm_examples/indepth_parallelizing_blastp/\ncurl -L -o TAIR10_pep.fasta  https://www.arabidopsis.org/download_files/Proteins/TAIR10_protein_lists/TAIR10_pep_20101214\n\n## we can prepare the data within a interactive job\nsrun -c 4 --mem=8G -p defq --pty bash\n\n## navigate to mydata directory\ncd ~/mydata/\n\n## make a dirctory for blast test\nmkdir -p blast_test && cd blast_test \n\n## copy TAIR file accross from class data to local file system\ncp /mnt/clusters/sponsa/data/classdata/Bioinformatics/REFS/slurm/slurm_examples/indepth_parallelizing_blastp/run2_split-files4/TAIR10_pep.fasta .\n\n## Makeblast db from downloaded file\nmodule load blast/2.12.0\nmakeblastdb -in TAIR10_pep.fasta -parse_seqids -dbtype prot -title \"Small protein database\" -out TAIR10_pep\n\n##copy the query sub-set\ncp /mnt/clusters/sponsa/data/classdata/Bioinformatics/REFS/slurm/slurm_examples/indepth_parallelizing_blastp/TAIR10_pep_4000.fasta .\n\nexit\n\nNow create the job Script - blastp.sh\n\n#!/bin/bash\n#SBATCH --partition=defq       # the requested queue\n#SBATCH --nodes=1              # number of nodes to use\n#SBATCH --tasks-per-node=1     # \n#SBATCH --cpus-per-task=1      #   \n#SBATCH --mem-per-cpu=92000     # in megabytes, unless unit explicitly stated\n#SBATCH --error=%J.err         # redirect stderr to this file\n#SBATCH --output=%J.out        # redirect stdout to this file\n##SBATCH --mail-user=[insert email address]@Cardiff.ac.uk  # email address used for event notification\n##SBATCH --mail-type=start                                 # email on job start  \n##SBATCH --mail-type=end                                   # email on job end\n##SBATCH --mail-type=fail                                  # email on job failure\n\necho \"Usable Environment Variables:\"\necho \"=============================\"\necho \\$SLURM_JOB_ID=${SLURM_JOB_ID} \necho \\$SLURM_NTASKS=${SLURM_NTASKS}\necho \\$SLURM_NTASKS_PER_NODE=${SLURM_NTASKS_PER_NODE}\necho \\$SLURM_CPUS_PER_TASK=${SLURM_CPUS_PER_TASK}\necho \\$SLURM_JOB_CPUS_PER_NODE=${SLURM_JOB_CPUS_PER_NODE}\necho \\$SLURM_MEM_PER_CPU=${SLURM_MEM_PER_CPU}\ncat $0\n\nmodule load blast/2.12.0\n\nindir=\"/mnt/clusters/sponsa/data/${USER}/blast_test\"\n\ndbdir=\"/mnt/clusters/sponsa/data/${USER}/blast_test\"\n\noutdir=\"/mnt/clusters/sponsa/data/${USER}/blast_test\"\n\ntime blastp -num_threads ${SLURM_CPUS_PER_TASK} \\\n            -query \"${indir}/TAIR10_pep_4000.fasta\" \\\n            -task blastp \\\n            -num_descriptions 16 \\\n            -num_alignments 1 \\\n            -db ${dbdir}/TAIR10_pep \\\n            -out \"${outdir}/blastp_cpu${SLURM_CPUS_PER_TASK}_job${SLURM_JOBID}.txt\"\n\n\nRun this blast script a number of times, increasing the –cpus-per-task and decreasing the –mem-per-cpu variables accordingly (ie 2 cpus-per-task & 46000 mem-per-cpu, 3 cpus-per-task & 30600 mem-per-cpu …..16 cpus-per-task & 5750 mem-per-cpu. Review the benchmarks considering the resources needed and the time taken to execute the jobs\nWhat are your conclusions",
    "crumbs": [
      "Session 2: HPC Cloud Slurm",
      "HPC, Cloud Computing & Job Schedulers"
    ]
  },
  {
    "objectID": "2.1_HPC_Cloud_Slurm.html#reviewing-and-optimising-job-resources",
    "href": "2.1_HPC_Cloud_Slurm.html#reviewing-and-optimising-job-resources",
    "title": "HPC, Cloud Computing & Job Schedulers",
    "section": "Reviewing and optimising job resources",
    "text": "Reviewing and optimising job resources\n\nUse gomphus.job_history.sh to review the efficiency of your job",
    "crumbs": [
      "Session 2: HPC Cloud Slurm",
      "HPC, Cloud Computing & Job Schedulers"
    ]
  },
  {
    "objectID": "2.1_HPC_Cloud_Slurm.html#parallel-jobs",
    "href": "2.1_HPC_Cloud_Slurm.html#parallel-jobs",
    "title": "HPC, Cloud Computing & Job Schedulers",
    "section": "Parallel Jobs",
    "text": "Parallel Jobs\nThere are a number of different scenarios in which you may want to parallelize your job:\n\nEmbarrassingly parallel\nMPI - a multi-process application (possibly across multiple compute hosts)\nmulti-threading - shared memory using OpenMP or perhaps pthreads.\nmultiple instances of a single application.\n…plus more scenarios but are probably out of scope of this tutorial.",
    "crumbs": [
      "Session 2: HPC Cloud Slurm",
      "HPC, Cloud Computing & Job Schedulers"
    ]
  },
  {
    "objectID": "2.1_HPC_Cloud_Slurm.html#embarrassingly-parallel",
    "href": "2.1_HPC_Cloud_Slurm.html#embarrassingly-parallel",
    "title": "HPC, Cloud Computing & Job Schedulers",
    "section": "Embarrassingly parallel",
    "text": "Embarrassingly parallel\nMany processes in genomics do the same task on large arrays of data. One of the simplest way of speeding up the process is to split up the input files and perfrom the same task multiple times at the same time - this is called an _‘Embarrassingly parallel’ task.\nLets do this for the blast example that we started in the last task. We can investigate whether there are any further methods of improving performance, and attempt to find a improvemet in the initial blast job’s solution to reduce the time taken. As it turns out, it is possible in this situation to split the input fasta file into a number of sections, and have an independent job acting on each of those sections. Each independent job could then be parallelized, say over 8 threads, and all jobs can run concurrently.\nWe create a job script (mammoth_8thread_split_1of4.sh) that will run blast command on the first file-section only.\nWe perform the following sequence of commands, first splitting the input fasta file into 4 parts, then creating the 4 independent job-scripts, and submit the jobs.\n\n## enter a interactive Job\nsrun -c 4 --mem=8G -p defq --pty bash\ncd ~/mydata/blast_test/\n\n## a new folder\nmkdir split-files4\ncd split-files4/\ncp ~/TAIR10_pep_4000.fasta  .\n#\n## split the fasta file into 4 equal(ish) sections\ncurl -L https://raw.githubusercontent.com/gmarnellos/Trinotate_example_supplement/master/split_fasta.pl | perl /dev/stdin  TAIR10_pep_4000.fasta  TAIR10_pep.vol 1000\n\ncreate script that will spawn blast_jobs (use nano or vi) - spawn_blastp.sh\n\n!#/bin/bash\n\nfor i in {1..4};do\n\nexport i\n\nsbatch blastp.sh\n\ndone\n\nmake script executable chmod +x spawn_blastp.sh\nEdit blast job so that each time it is called it uses a different section of the query file\n\n#!/bin/bash\n#SBATCH --partition=defq       # the requested queue\n#SBATCH --nodes=1              # number of nodes to use\n#SBATCH --tasks-per-node=1     # \n#SBATCH --cpus-per-task=8      #   \n#SBATCH --mem-per-cpu=11500    # in megabytes, unless unit explicitly stated\n#SBATCH --error=%J.err         # redirect stderr to this file\n#SBATCH --output=%J.out        # redirect stdout to this file\n##SBATCH --mail-user=[insert email address]@Cardiff.ac.uk  # email address used for event notification\n##SBATCH --mail-type=start                                 # email on job start  \n##SBATCH --mail-type=end                                   # email on job end\n##SBATCH --mail-type=fail                                  # email on job failure\n\necho \"Usable Environment Variables:\"\necho \"=============================\"\necho \"hostname=$(hostname)\"\necho \\$SLURM_JOB_ID=${SLURM_JOB_ID} \necho \\$SLURM_NTASKS=${SLURM_NTASKS}\necho \\$SLURM_NTASKS_PER_NODE=${SLURM_NTASKS_PER_NODE}\necho \\$SLURM_CPUS_PER_TASK=${SLURM_CPUS_PER_TASK}\necho \\$SLURM_JOB_CPUS_PER_NODE=${SLURM_JOB_CPUS_PER_NODE}\necho \\$SLURM_MEM_PER_CPU=${SLURM_MEM_PER_CPU}\n\ncat $0\n\nmodule load blast/2.12.0\n\nindir=\"/mnt/clusters/sponsa/data/${USER}/blast_test/split-files4\"\n\ndbdir=\"/mnt/clusters/sponsa/data/${USER}/blast_test\"\n\noutdir=\"/mnt/clusters/sponsa/data/${USER}/blast_test/split-files4\"\n\ntime blastp -num_threads ${SLURM_CPUS_PER_TASK} \\\n            -query \"${indir}/TAIR10_pep.vol.${i}.fasta\" \\\n            -task blastp \\\n            -num_descriptions 16 \\\n            -num_alignments 1 \\\n            -db ${dbdir}/TAIR10_pep \\\n            -out \"${outdir}/blastp_vol${i}_cpu${SLURM_CPUS_PER_TASK}_job${SLURM_JOBID}.txt\"\n\nPerform file-splitting procedure for both a 2-split and a 4-split of the original fasta file. The ‘time-to-solution’ results are added to the original benchmark chart. We assume that all jobs run concurrently, and we take the wall-time for the longest job",
    "crumbs": [
      "Session 2: HPC Cloud Slurm",
      "HPC, Cloud Computing & Job Schedulers"
    ]
  },
  {
    "objectID": "2.1_HPC_Cloud_Slurm.html#mpi-jobs",
    "href": "2.1_HPC_Cloud_Slurm.html#mpi-jobs",
    "title": "HPC, Cloud Computing & Job Schedulers",
    "section": "MPI Jobs",
    "text": "MPI Jobs\nOur example MPI job is based on a quantum espresso calculation. This script utilises the srun command, which is part of the slurm family of tools to run a parallel job on a cluster\n\n#!/bin/bash\n#SBATCH --partition=mammoth    # the requested queue\n#SBATCH --job-name=qe_mpi      # name the job           \n#SBATCH --nodes=1              # number of nodes to use\n#SBATCH --ntasks=32            # total number of tasks (processes)\n#SBATCH --mem-per-cpu=100      # in megabytes, unless unit explicitly stated\n#SBATCH --error=%J.err         # redirect stderr to this file\n#SBATCH --output=%J.out        # redirect stdout to this file\n##SBATCH --mail-user=[insert email address]@Cardiff.ac.uk  # email address used for event notification\n##SBATCH --mail-type=end                                   # email on job end\n##SBATCH --mail-type=fail                                  # email on job failure\n\nmodule load  qe/6.0\n\necho \"Usable Environment Variables:\"\necho \"=============================\"\necho \"hostname=$(hostname)\"\necho \\$SLURM_JOB_ID=${SLURM_JOB_ID}\necho \\$SLURM_NTASKS=${SLURM_NTASKS}\necho \\$SLURM_NTASKS_PER_NODE=${SLURM_NTASKS_PER_NODE}\necho \\$SLURM_CPUS_PER_TASK=${SLURM_CPUS_PER_TASK}\necho \\$SLURM_JOB_CPUS_PER_NODE=${SLURM_JOB_CPUS_PER_NODE}\necho \\$SLURM_MEM_PER_CPU=${SLURM_MEM_PER_CPU}\necho \"module list:\"\nmodule list 2&gt;&1\n\n# Some of these environment variables are utilised by the qe executable itself\nexport ESPRESSO_DATAPATH=/mnt/clusters/sponsa/data/classdata/Bioinformatics/REFS/slurm/slurm_examples/example2_mpi/\nexport ESPRESSO_PSEUDO=${ESPRESSO_DATAPATH}\nexport ESPRESSO_TMPDIR=${ESPRESSO_DATAPATH}/${SLURM_JOB_ID}\n\n# handy to place this in job output for future reference...\ncat ${ESPRESSO_DATAPATH}/atom.in\n\n# execute the parallel job (we also time it)\ntime srun -n ${SLURM_NTASKS} pw.x &lt; ${ESPRESSO_DATAPATH}/atom.in &gt; atom.job${SLURM_JOB_ID}.out\n\nThe job requests 32 cores to be allocated, and runs the srun command with the argument -n ${SLURM_NTASKS} which tells srun to spawn the mpi-job with the total number of processes requested. Quantum Espresso utilises the environment variable ESPRESSO_TMPDIR which points to a temporary folder. We design this in our slurm script to point to a subfolder.\nAn alternative storage location is the compute node’s local storage. This can improve runtime I/O performance. However, local storage on the compute nodes is limited (Gigabytes not Terabytes), and it’s availability is a little hidden from the user, so take care not to fill up the disk(!) and remove all files from the compute node’s local storage within the job script (your only access to the compute node’s /local folder is via the slurm script). An alternative job script which utilises a compute node’s /local storage is provided on the gomphus server /mnt/clusters/sponsa/data/classdata/Bioinformatics/REFS/slurm/slurm_examples/example2_mpi/example2_mpi_localstorage.sh.",
    "crumbs": [
      "Session 2: HPC Cloud Slurm",
      "HPC, Cloud Computing & Job Schedulers"
    ]
  },
  {
    "objectID": "2.1_HPC_Cloud_Slurm.html#threaded-jobs",
    "href": "2.1_HPC_Cloud_Slurm.html#threaded-jobs",
    "title": "HPC, Cloud Computing & Job Schedulers",
    "section": "Threaded Jobs",
    "text": "Threaded Jobs\nA number of popular bioinformatics software are capable of parallelising execution using threads (usually OpenMP or pthreads). This parallelisation method does not normally use distributed memory, so the application will need to be run on a single node. Our threaded example slurm-script is based on BLAST+. The job script is listed:\n\n#!/bin/bash\n#SBATCH --partition=defq\n#SBATCH --nodes=1\n#SBATCH --tasks-per-node=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem-per-cpu=2000\n#SBATCH --error=%J.err\n#SBATCH --output=%J.out\n##SBATCH --mail-type=end\n##SBATCH --mail-user=[your.email@address]\n\n\n# Example slurm script\n#  This script is a little wasteful of resources,\n#  but demonstrates a simple pipeline.\n#  \n#  For a more efficient use of resources, please consider\n#  running the pipeline as a series of jobs (chain-jobs).\n\n\necho \"Usable Environment Variables:\"\necho \"=============================\"\necho \"hostname=$(hostname)\"\necho \\$SLURM_JOB_ID=${SLURM_JOB_ID}\necho \\$SLURM_NTASKS=${SLURM_NTASKS}\necho \\$SLURM_NTASKS_PER_NODE=${SLURM_NTASKS_PER_NODE}\necho \\$SLURM_CPUS_PER_TASK=${SLURM_CPUS_PER_TASK}\necho \\$SLURM_JOB_CPUS_PER_NODE=${SLURM_JOB_CPUS_PER_NODE}\necho \\$SLURM_MEM_PER_CPU=${SLURM_MEM_PER_CPU}\necho \"module list:\"\nmodule list 2&gt;&1\n\nDATAFOLDER=/mnt/clusters/sponsa/data/classdata/Bioinformatics/REFS/slurm/slurm_examples/example3_pipeline\n\n\n### The data used in this pipeline has already been downloaded and stored in $DATAFOLDER.\n### Here are the commands used to download the data...\n# cd $DATAFOLDER\n# curl -LO https://data.broadinstitute.org/Trinity/Trinotate_v2.0_RESOURCES/uniprot_sprot.trinotate_v2.0.pep.gz\n# gunzip uniprot_sprot.trinotate_v2.0.pep.gz\n# curl -LO https://data.broadinstitute.org/Trinity/Trinotate_v2.0_RESOURCES/Pfam-A.hmm.gz\n# gunzip Pfam-A.hmm.gz\n# curl -L -o Trinotate.sqlite.gz https://data.broadinstitute.org/Trinity/Trinotate_v2.0_RESOURCES/Trinotate.sprot_uniref90.20150131.boilerplate.sqlite.gz\n# gunzip Trinotate.sqlite.gz\n# curl -LO ftp://ftp.ensembl.org/pub/release-82/fasta/mus_musculus/cdna/Mus_musculus.GRCm38.cdna.all.fa.gz\n# gunzip Mus_musculus.GRCm38.cdna.all.fa.gz\n# curl -LOgz https://github.com/gmarnellos/Trinotate_example_supplement/raw/master/mouse38_cdna.fa.gz\n# gunzip mouse38_cdna.fa.gz\n\n# Now we get on to the pipeline\n\n# make a link to all datafiles\nfor f in ${DATAFOLDER}/* ; do ln -s $f ; done\n\n#Index the SwissProt database for use with blast\n\nmodule load blast\nmakeblastdb -version\nmakeblastdb -in uniprot_sprot.trinotate_v2.0.pep -dbtype prot\nmodule unload blast\n\n# Prepare the Pfam database for use with hmmscan\nmodule load hmmer\nhmmpress -h\nhmmpress Pfam-A.hmm\nmodule load hmmer\n\n# Use Transdecoder to produce the most likely longest-ORF peptide candidates\nmodule load TransDecoder/v3.0.1\nTransDecoder.LongOrfs -t mouse38_cdna.fa\nTransDecoder.Predict -t mouse38_cdna.fa\nmodule unload TransDecoder/v3.0.1\n\nmodule load blast\nblastx -query mouse38_cdna.fa -db uniprot_sprot.trinotate.pep -num_threads ${SLURM_CPUS_PER_TASK} -max_target_seqs 1 -outfmt 6 &gt; blastx.vol.outfmt6\nblastp -query mouse38_cdna.fa.transdecoder.pep -db uniprot_sprot.trinotate_v2.0.pep -num_threads ${SLURM_CPUS_PER_TASK} -max_target_seqs 1 -outfmt 6 &gt; blastp.vol.outfmt6\nmodule unload blast\n\n# Identify protein domains\nmodule load hmmer/3.1b2\nhmmscan --cpu ${SLURM_CPUS_PER_TASK} --domtblout TrinotatePFAM.out Pfam-A.hmm mouse38_cdna.fa.transdecoder.pep &gt; pfam.log\nmodule unload hmmer/3.1b2\n\n# Produce the Gene/Transcript relationship\ngrep \"^&gt;\" Mus_musculus.GRCm38.cdna.all.fa   | perl -p -e 's/^&gt;(\\S+).*\\s+gene:(ENSMUSG\\d+).*$/$2\\t$1/' &gt; gene_transcript_map.txt\n\n# Now populate the sqlite database\nmodule load Trinotate/v3.0.1\nTrinotate Trinotate.sqlite init --gene_trans_map gene_transcript_map.txt --transcript_fasta mouse38_cdna.fa --transdecoder_pep mouse38_cdna.fa.transdecoder.pep\nTrinotate Trinotate.sqlite LOAD_swissprot_blastp blastp.vol.outfmt6\nTrinotate Trinotate.sqlite LOAD_swissprot_blastx blastx.vol.outfmt6\nTrinotate Trinotate.sqlite LOAD_pfam TrinotatePFAM.out\n# Create the annotation report\nTrinotate Trinotate.sqlite report -E 0.00001 &gt; trinotate_annotation_report.xls\nmodule unload Trinotate/v3.0.1\n\nThis is quite a busy job-script (and also inefficient on resources!). It runs through a number of steps, but some of those steps will utilise parallelisation via threading, and use the slurm environment variable SLURM_CPUS_PER_TASK to inform the application(s) of the correct number of threads.\nBut why is this job inefficient on resources? This particular job involves a number of steps: some utilising parallelisation, and some not; some memory-hungry, others not. The problem with this is that the job has allocated to it a set amount of resources (compute and memory), which is allocated to it for the lifetime of the job. But only at certain times in this job are the resources requested fully utilised. At all other times this job is running, the resources are allocated, but not used, and therefore making those resources unavailable to other jobs. This has a knock-on effect of increasing queue-times, and leaves expensive resources idle.\nA much more efficient way of running the same pipeline is to chain the job - split the pipeline into component parts and submit separate jobs for each of those parts. Each section of the pipeline (having its own job-script) is then free to allocate resources specific to that section of the pipeline. In the slurm world this is called job chaining, and has been exemplified in the next section using the same pipeline.",
    "crumbs": [
      "Session 2: HPC Cloud Slurm",
      "HPC, Cloud Computing & Job Schedulers"
    ]
  },
  {
    "objectID": "2.1_HPC_Cloud_Slurm.html#job-chains-and-job-dependency",
    "href": "2.1_HPC_Cloud_Slurm.html#job-chains-and-job-dependency",
    "title": "HPC, Cloud Computing & Job Schedulers",
    "section": "Job Chains and Job Dependency",
    "text": "Job Chains and Job Dependency\nChaining jobs is a method of sequentially running dependent jobs. Our chain-job example is a pipeline of 6 separate job scripts, based on the blast+ pipeline of the previous section. We do not show the full six job-scripts here for brevity, but are available on the gomphus cluster under /mnt/clusters/sponsa/data/classdata/Bioinformatics/REFS/slurm/slurm_examples/example4_chain.\nSlurm has an option -d or –dependency that allows to specify that a job is only permitted to start if another job finished successfully.\nIn the folder (gomphus cluster) /mnt/clusters/sponsa/data/classdata/Bioinformatics/REFS/slurm/slurm_examples/example4_chain there are 6 separate job-scripts that need to be executed in a certain order. They are numbered in the correct pipeline order:\n\n[user@gomphus ~]$ tree  /mnt/clusters/sponsa/data/classdata/Bioinformatics/REFS/slurm/slurm_examples/example4_chain\n/mnt/clusters/sponsa/data/classdata/Bioinformatics/REFS/slurm/slurm_examples/example4_chain\n├── example4_chain-step1.sh\n├── example4_chain-step2.sh\n├── example4_chain-step3.sh\n├── example4_chain-step4.sh\n├── example4_chain-step5.sh\n├── example4_chain-step6.sh\n├── example4_submit_all.sh\n├── mouse38_cdna.fa\n├── Mus_musculus.GRCm38.cdna.all.fa\n├── Pfam-A.hmm\n├── pipeline1.sh\n├── Trinotate.sqlite\n└── uniprot_sprot.trinotate_v2.0.pep\n\n0 directories, 13 files\n\nEach job is (importantly) commonly named using #SBATCH –job-name within each job-script. Also within this folder is a simple script (example4_submit_all.sh) that will execute the sbatch command on each of the job-scripts in the correct order:\n\n#!/bin/bash:\n\nfor c in /mnt/clusters/sponsa/data/classdata/Bioinformatics/REFS/slurm/slurm_examples/example4_chain/example4_chain-step?.sh ;\ndo\n sbatch -d singleton $c\ndone\n\nThis sbatch command uses the -d singleton flag to notify slurm of the job-dependencies (all jobs must have the name job name defined by #SBATCH --job-name [some constant name]. At which point each submitted job will be forced to depend on successful completion of any previous job submitted by the same user, and with the same job-name. The full pipeline of 6 jobs will now run to completion, with no further user-intervention, making efficient use of the available resources.",
    "crumbs": [
      "Session 2: HPC Cloud Slurm",
      "HPC, Cloud Computing & Job Schedulers"
    ]
  },
  {
    "objectID": "0.1_index.html",
    "href": "0.1_index.html",
    "title": "Big Data Science",
    "section": "",
    "text": "Big Data Science - BIT107\nThe material associated with this git hub repository is workshop instructions linked to the Big Data Science Module BIT107, integral module for the Msc in Big Data Biology hosted by Cardiff School of Biosciences, Cardiff University. If you would like to know more about our course or any other masters course at Cardiff University please visit our website - Big Data Biology (MSc).",
    "crumbs": [
      "Big Data Biology - BIT010",
      "Big Data Science"
    ]
  },
  {
    "objectID": "2.2_Slurm_dependencies.html#section",
    "href": "2.2_Slurm_dependencies.html#section",
    "title": "Slurm Dependencies",
    "section": "",
    "text": "Component scripts start:\n\n#!/bin/bash\n#SBATCH --job-name=test_workflow\n#SBATCH --partition=defq       # the requested queue\n#SBATCH --nodes=1              # number of nodes to use\n#SBATCH --tasks-per-node=1     #\n#SBATCH --cpus-per-task=1      #   \n#SBATCH --mem-per-cpu=1000     # in megabytes, unless unit explicitly stated\n#SBATCH --error=%J.err         # redirect stderr to this file\n#SBATCH --output=%J.out        # redirect stdout to this file\n......\n\nand your workflow scripts \n#!/bin/bash\n\nsbatch -d singleton [slurm_script1]\n\nsbatch -d singleton [slurm_script2]\n\n\n\n\n\n\n\nExercise: Make your instance based transcriptomic processes slurm compatible WORKFLOW\n\n\n\nIn Session 5 of you bioinformatics introductory lectures you were given scripts to pre-process, map, count and call duplicates - RNAseq Processing. I want you to transfer these and the data to scratch and convert these to slurm compatible scripts.\nLEAVE OUT STAR LIBRARY FORMTTING STEP",
    "crumbs": [
      "Session 2: HPC Cloud Slurm",
      "Slurm Dependencies"
    ]
  },
  {
    "objectID": "3.1_FAIR_data.html#guide-to-contributing-to-ncbi-sra",
    "href": "3.1_FAIR_data.html#guide-to-contributing-to-ncbi-sra",
    "title": "FAIR Data",
    "section": "Guide to contributing to NCBI SRA",
    "text": "Guide to contributing to NCBI SRA\nAre you looking to make your science more reproducible and transparent by making your data public? Or do you need to upload your data somewhere before your manuscript can be published? The NCBI Sequence Reach Archive (SRA) is a public searchable platform where genetic data and associated metadata may be uploaded, downloaded and reused.\n\nGetting started:\nBefore beginning, it is helpful to read through several NCBI pages to understand the necessary data that is required for a successful upload. A good starting point is here. Understanding NCBI’s data hierarchy will also be helpful for understanding how your data should be submitted.\n####A summary of NCBI prefixes####\n\n\n\nPrefix\nAccession Name\nNCBI Definition\nExample\n\n\n\n\nPRJNA\nBioProject\nThe goal of your research effort\nPRJNA477007\n\n\nSRP\nStudy\nAn object that contains project metadata describing sequencing study or project\nSRP150953\n\n\nSAMN\nSample\nAn object that contains metadata describing the physical sample upon which a sequencing experiment was performed\nSAMN09463455\n\n\nSRX\nExperiment\nAn object containing metadata describing the library, platform selection and processing parameters involved in a sequencing experiment\nSRX7621456\n\n\nSRR\nRun\nAn object containing actual sequencing data for a sequencing experiment. Experiments may contain multiple runs if multiple sequencing instrument runs were needed, but preferable data structure is one run per experiment\nSRR10954732\n\n\n\n\n\nSubmitting:\nLog in on to the SRA Submission Portal Wizard\nCreate new submission by clicking on the ‘New Submission’ button. Your submission will receive a temporary SUB# ID, and you can use this to contact SRA staff if you have issues.\n#####Follow steps (you can leave at any step and return to it later). The Submission Portal will check to make sure everything is okay after each step and your position will be displayed on the progress bar.\n Figure 1. NCBI SRA submission progress bar\n1. Submitter information\nYou will be asked for your contact information and affiliations. You can also create a Group, which will allow your collaborators to read, modify, submit and delete your submissions.\n2. General information\nYou will be asked if you already registered your project and samples. Select yes or no. If yes, enter the accession number for your existing BioProject and you will be redirected to Step #6. If no, the Wizard will ask you to create them. Select the release date for your data. The default is immediately.\n3. Project information\nEnter information about your project, including title, a description and grant information.\n4. BioSample type\nSelect the best description for your data. A Pinsky Lab example would be ‘Model organism or animal sample’.\n5. BioSample attributes\nYou will be asked to provide information about your samples. You can either enter it directly into the built-in editor, or download a BioSamples template, fill it out and upload it.\nEnter each sample as a separate line and follow the directions on the template with regards to each of the colored data columns to ensure all necessary data have been included.\nA complete BioSamples template can be viewed here.\n6. SRA metadata\nIf you are entering metadata for new BioSamples, you may either enter the metadata using the built-in editor, or you may download a SRA metadata template, fill it out and upload it. Follow the instructions on the template to ensure all required information about sequencing methodology is included. An example of a SRA metadata template for new BioSamples may be viewed here.\nIf you have previously uploaded SRA metadata, download and complete the SRA metadata template. You must make sure that you include the BioSample accession number so that your new sequences are correctly linked to your existing BioSamples. An easy way to obtain these is to navigate to your previous submission within the submission portal and then to Download the attributes file with BioSample accessions (Figure 2).\n\n\n\nalt text\n\n\nFigure 2. Where to download a file with BioSample accession numbers for previously submitted samples.\n7. Files\nNCBI SRA accepts different file types. The Pinsky Lab aims to contribute FASTQ and BAM files for each sequenced individual, plus the reference that reads are aligned to. More information about file types may be found here. The file names need to be the same as those that you specified in the SRA Metadata.\nAs genomic data are large, there are several ways to transfer your sequence data to the NCBI SRA. Within this section of the Submission Portal, NCBI offers several ways to transfer your data: HTTP/Aspera Connect Plugin, FTP/Aspera Command Line or Amazon S3. If your data are less than 10 GB or you have fewer than 300 individuals, you can try to drag files directly into the Submission Portal, which will upload via HTTP. If your data are larger, select Request preload folder button. If you select the method you would like to use, some brief instructions will appear. Here, we will discuss various FTP options in more detail. Note that the time it takes to transfer your data will depend on the size of your data (often hours to days).\nFTP using the command line\n\nCreate a single directory with all the files you want to upload.\nEnsure you have FTP. You may need to install if not.\nNavigate to the directory all the files you want to upload are.\nEstablish a FTP connection by typing ftp -i.\nNext, type open ftp-private.ncbi.nlm.nih.gov.\nProvide your username this is listed in the Submission Portal, likely subftp.\nProvide the password listed in the Submission Portal.\nNavigate to your account folder listed in the Submission Portal: cd uploads/your_account_folder_name\nCreate a subfolder. You must do this or you will not be able to see your files in the preload option: mkdir new_folder_name\nNavigate into the target folder: cd new_folder_name\nCopy your files into the target folder: mput *\nGo back to the Submission Portal and select the folder to upload. It takes at least 10 minutes for transferred files to appear in the preload option.\nTo exit the FTP window, type bye.\n\nFTP using third-party software (Fetch, for example)\n\nOpen a Fetch window and connect to the NCBI server by typing in the hostname, your username and password, and direct Fetch to your new_folder_name within your_account_folder_name (Figure 3).\nOpen another Fetch window and connect to the location of your data.\nHighlight the files you want to transfer and drag into the Fetch window that is connected to the NCBI server. This will copy your files from your server to the NCBI server.\nGo back to the Submission Portal and select the folder to upload. It takes at least 10 minutes for transferred files to appear in the preload option.\n\n Figure 3. Screenshot showing how to connect to the NCBI server using Fetch\n8. Review & submit\nThe Submission Portal will check to make sure all your sequencing files have correctly transferred. It will also check to make sure that you have uploaded sequence data for all listed BioSamples. Review all the information before completing the submission. If an error arises during processing, you’ll receive an email asking you to contact SRA staff.\n\n\nTroubleshooting:\nIf you run into trouble, there is a SRA Submission Portal Troubleshooting Guide, or email SRA staff at sra@ncbi.nlm.nih.gov",
    "crumbs": [
      "Session 3: FAIR Data",
      "FAIR Data"
    ]
  },
  {
    "objectID": "3.1_FAIR_data.html#review-sra-submission-process-by-reviewing-submission-srx8525144",
    "href": "3.1_FAIR_data.html#review-sra-submission-process-by-reviewing-submission-srx8525144",
    "title": "FAIR Data",
    "section": "Review SRA submission process by reviewing submission SRX8525144",
    "text": "Review SRA submission process by reviewing submission SRX8525144\nIdentify the following data about this submission:\n\nAuthor\nOrganism (full taxonomy)\nSequencing platform / type\nBiomaterial, Bioproject\nPublication and aim of work",
    "crumbs": [
      "Session 3: FAIR Data",
      "FAIR Data"
    ]
  },
  {
    "objectID": "3.1_FAIR_data.html#creating-a-zenodo-account",
    "href": "3.1_FAIR_data.html#creating-a-zenodo-account",
    "title": "FAIR Data",
    "section": "Creating a Zenodo Account",
    "text": "Creating a Zenodo Account\nStep 1. Signup To create an account with Zenodo click on the signup tab at there home page https://zenodo.org/.\n\n\n\nzenodo signup page\n\n\nStep 2. Link to Github (or ORCID) You will be provided with options to link your submission to either your ORCID account or your Github account. If you are a academic linking to you ORCID account has the advantage that it tracks your Zenodo submissions to your ORCID profile so people can immediately view your public accessions. For today’s session I suggest you link the account to your github account created earlier.\n\n\n\nLink to Github\n\n\nStep 3. Zenodo Authorization form Zenodo needs you to agree to various data sharing protocols - review and accept if you are happy to do so.\n Step 4. Data Upload I suggest you upload your ICA assigned sequences.\n\n\n\nUpload Page\n\n\nStep 5. Zenodo Big Data Biology Master Community I have created a community that you can associated your data with so we can see the class submissions.\n\n\n\nCardiff University - Big Data Biology Masters\n\n\nStep 6. Submission Pages Here’s is a minimal metadata need to support the submission. I suggest you keep the submission as closed access submission as you will uploading non-primary data.\n\n\n\nSubission Page 1\n\n\n\n\n\nSubission Page 2\n\n\n\n\n\nSubission Page 3\n\n\nStep 7. Review Submission Review the submission -\n\n\n\nReview Submission\n\n\n\nZenodo\n\nCreate an account\nUse a dataset to create a private repository\nLink repository to your github page\nCreate a private submission linked to cu-bdbm community",
    "crumbs": [
      "Session 3: FAIR Data",
      "FAIR Data"
    ]
  },
  {
    "objectID": "4.1_RStudio_container.html",
    "href": "4.1_RStudio_container.html",
    "title": "Rstudio_container",
    "section": "",
    "text": "Rstudio Quick and easy\n\nStep 1 - Copy Rstudio_container folder from classdata into you mydata directory\nStep 2 - Make sure you have bash local environment installed on mobxterm (PC only)\nStep 3 - Change directory to enter Rstudio_container folder and run sbatch rstudio-server-442.job\nStep 4 - Print [jobid].err file to screen - note the ssh command and copy and paste that into a ‘local’ blank terminal session.\nStep 5 - Paste http://localhost:8787 into a browser and copy and paste in username and password listed in the [jobid].err file.\nStep 6 - Local interface to a server based Rstudio should now appear. You can close browser and relogon as many times as you like. The session will close in 24 hours unless you edit the job file (see advanced instructions). You also do not need to keep your command line session running.\nStep 7 - When finished please scancel -f [jobid]",
    "crumbs": [
      "Session 4: RStudio Container",
      "Rstudio_container"
    ]
  },
  {
    "objectID": "Images/SRA-submission-master/data-prep.html",
    "href": "Images/SRA-submission-master/data-prep.html",
    "title": "R Notebook",
    "section": "",
    "text": "Prepping data to add to SRA\n\nImport the template\nThe first 11 rows are instructions\n\ntemplate &lt;- read_tsv(\"Model.organism.animal.1.0.tsv\", skip = 11)\n\n\n\nImport the data\n\nligation_ids &lt;- read_genepop(\"~/Documents/genomics_pinskylab/data/seq33_03_baits_only_SNPs.gen\") %&gt;% \n  # choose only the names column\n  select(sample) %&gt;% \n  # remove APCL_ from the ligation_id\n  mutate(ligation_id = str_extract(sample, \"L\\\\d+\")) %&gt;% \n  # remove the names column\n  select(-sample)\n\n# bring in fish data from database\nfish &lt;- fish_anem_dive() %&gt;% \n  filter(!is.na(sample_id)) %&gt;% \n  select(sample_id, sex, date)\n\n#what are the sample_ids for these ligation_ids\nsample_ids &lt;- samp_from_lig(ligation_ids) %&gt;% \n  rename(sample_name = sample_id) %&gt;% \n  # Add the organism\n  mutate(organism = \"Amphiprion clarkii\", \n         isolate = \"not applicable\", \n         tissue = \"tail fin\") %&gt;% \n  left_join(fish, by = c(\"sample_name\" = \"sample_id\")) %&gt;% \n  rename(collection_date = date)\n\n\n\nSRA did not like repeat sample_ids. Removing sample_ids and keeping only ligation ids\n\nlig_only &lt;- sample_ids %&gt;% \n  select(-sample_name) %&gt;% \n  rename(sample_name = ligation_id)\n\n\n\nApparently there are also duplicate ligation_ids\n\ndups &lt;- sample_ids %&gt;% \n  group_by(ligation_id) %&gt;% \n  count() %&gt;% \n  filter(n &gt; 1)\n\n\n\nAccording to this code, there are not duplicates. However, the wording looks like maybe they want unique individuals, as in individuals that were not collected on the same day.\n“Your table upload failed because multiple BioSamples cannot have identical attributes. You should have one BioSample for each specimen, and each of your BioSamples must have differentiating information (excluding sample name, title, bioproject accession and description). This check was implemented to encourage submitters to include distinguishing information in their samples. If the distinguishing information is in the sample name, title or description, please recode it into an appropriate attribute, either one of the predefined attributes or a custom attribute you define. If it is necessary to represent true biological replicates as separate BioSamples, you might add an ‘aliquot’ or ‘replicate’ attribute, e.g., ‘replicate = biological replicate 1’, as appropriate. Note that multiple assay types, e.g., RNA-seq and ChIP-seq data may reference the same BioSample if appropriate.” # To get past this, I am going to include ligation_id in sample name and also include a column called ligation_id (a custom attribute).\n\nsamp_lig &lt;- sample_ids %&gt;% \n  mutate(sample_name = str_c(sample_name, ligation_id, sep = \"_\"))\n\n\n\nSRA requires that we change M to male and F to female and that we define age, development stage\n\nsamp_lig &lt;- samp_lig %&gt;% \n  mutate(sex = ifelse(sex == \"M\", \"male\", sex), \n         sex = ifelse(sex == \"F\", \"female\", sex), \n         sex = ifelse(sex == \"J\", \"not applicable\", sex), \n         dev_stage = ifelse(sex == \"not applicable\", \"juvenile\", NA),\n         dev_stage = ifelse(sex != \"not applicable\", \"adult\", dev_stage)) %&gt;% \n  # remove any non_apcl samples\n  filter(grepl(\"APCL\", sample_name)) %&gt;% \n  arrange(ligation_id)\n\n\n\nSRA requires that these files are 1000 lines or less\n\ntable1 &lt;- samp_lig %&gt;% \n  slice(1:999)\ntable2 &lt;- samp_lig %&gt;% \n  slice(1000:1998)\ntable3 &lt;- samp_lig %&gt;% \n  slice(1998:2881)\n\n\n\nExport the data in tab delimited format\n\nwrite_tsv(table1, \"amphiprion-clarkii-table1.tsv\")\nwrite_tsv(table2, \"amphiprion-clarkii-table2.tsv\")\nwrite_tsv(table3, \"amphiprion-clarkii-table3.tsv\")\n\n\n\nLog on to https://submit.ncbi.nlm.nih.gov/subs/sra/SUB4607463/attributes and upload the table, then hit continue.\n\n\nThe next step is to add metadata\nView the SRA_metadata_PADE.xlsx file in excel\n\n# template2 &lt;- readxl::read_xlsx(\"SRA_metadata_PADE.xlsx\")\n\nSome samples have a different name and need to be replaced\n\n# open the list of files that have weird names\nreplacements &lt;- read_lines(\"list.txt\") %&gt;% \n  str_extract(., \"APCL_L\\\\d+.F-RG.bam\") %&gt;% \n  tibble() %&gt;% \n  rename(sample = \".\") %&gt;% \n  mutate(replace = str_c(str_extract(.,\"APCL_L\\\\d+\"), \"-RG.bam\"))\n\n\n\ncreate table for clarkii data\n\nmetadata &lt;- samp_lig %&gt;% \n  rename(library_ID = ligation_id) %&gt;% \n  mutate(title = \"ddRADseq of Amphiprion clarkii\", \n         library_strategy = \"RAD-Seq\", \n         library_source = \"GENOMIC\", \n         library_selection = \"Reduced Representation\", \n         library_layout = \"single\", \n         platform = \"Illumina\", \n         instrument_model = \"Illumina HiSeq 2500\", \n         design_description = \"ddRADseq using PstI and MluCI, and size selected to 375 ± 38 bp\", \n         filetype = \"bam\", \n         filename = str_c(\"APCL_\", library_ID, \"-RG.bam\", sep = \"\"), \n         assembly = NA\n         ) %&gt;%\n  select(-organism:-dev_stage) %&gt;% \n  # fix names of regeno files\n  mutate(filename = ifelse(filename == \"APCL_L0306-RG.bam\", \"APCL_L0306.L3598-RG.bam\", filename), \n          filename = ifelse(filename == \"APCL_L0308-RG.bam\", \"APCL_L0308.L3599-RG.bam\", filename),\n         filename = ifelse(filename == \"APCL_L0309-RG.bam\", \"APCL_L0309.L3601-RG.bam\", filename),\n         filename = ifelse(filename == \"APCL_L0814-RG.bam\", \"APCL_L0814.L3644-RG.bam\", filename),\n         filename = ifelse(filename == \"APCL_L1188-RG.bam\", \"APCL_L1188.L3266.L3352-RG.bam\", filename),\n         filename = ifelse(filename == \"APCL_L2324-RG.bam\", \"APCL_L2324.L3400-RG.bam\", filename)) %&gt;% \n  mutate(filename = ifelse(filename %in% replacements$replace, str_c(\"APCL_\", library_ID, \"-RG.bam\"), filename))\n\n\n\nSRA requires that these files are 1000 lines or less\n\nmetadata1 &lt;- metadata %&gt;% \n  slice(1:999)\nmetadata2 &lt;- metadata %&gt;% \n  slice(1000:1998)\nmetadata3 &lt;- metadata %&gt;% \n  slice(1998:2886)\n\n\n\nExport the meta data tables\n\nwrite_tsv(metadata1, \"amphiprion-clarkii-metadata1.tsv\")\nwrite_tsv(metadata2, \"amphiprion-clarkii-metadata2.tsv\")\nwrite_tsv(metadata3, \"amphiprion-clarkii-metadata3.tsv\")\n\n\n\nSending the bam files by FTP - COMPRESS Folder INTO TAR!!!\nI logged into the FTP using Fetch following the onscreen instructions and started dragging over bam files.\nHowever, the bam.bai files are also intertwined, it was taking hours, and was timing out before completing all of the transfers.\nI copied all of the bam files into a bam files folder in the data &gt; apcl &gt; all_samples &gt; 20181127 &gt; bam files.\nMaybe I can separate them into folders on my end and then just drag and drop the folder for submission. The folder transfer will take hours but hopefully it won’t time out.\n\n\nThe file names don’t match the metadata\nhand edit the metadata table because it is just a few regenotypes\n\n\nThe first submission is called SUB4607463\nThere is an error of undefined origin, waiting for an email response\n\n\nThe second submission is called SUB6249894\n\nMark no for biosample\nupload table_2 for biosample attributes\ntransferring files starting 11:20am on Monday, 9-9-2019"
  }
]